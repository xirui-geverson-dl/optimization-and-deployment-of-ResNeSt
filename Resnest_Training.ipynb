{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnest_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF98nGpcoVo-",
        "outputId": "82769585-2a93-41ad-8763-a5bdd1f9ba1f"
      },
      "source": [
        "cd drive/MyDrive/Colab\\ Notebooks/CS3033\\ Final\\ Project/results_xf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1ys85tT914ph5XYcchx1AzNxqHH3eO0Qe/CS3033 Final Project/results_xf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4rWe5qWo0XQ",
        "outputId": "90c8593e-a6d0-4b24-c436-68fc39b08a8a"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                          resnest50_bottleneck.pt\n",
            "resnest50_bottleneck.csv       resnest50_bottleneck_v100.csv\n",
            "resnest50_bottleneck_p100.csv  resnest50_bottleneck_v100.pt\n",
            "resnest50_bottleneck_p100.pt   transfer_bottleneck.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjJPJVXzWWk9"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from .splat import SplAtConv2d, DropBlock2D\n",
        "# from .build import RESNEST_MODELS_REGISTRY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhTp1m28gLRD"
      },
      "source": [
        "## Split attention convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJV8gQNdYLIT"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Conv2d, Module, Linear, BatchNorm2d, ReLU\n",
        "from torch.nn.modules.utils import _pair\n",
        "\n",
        "__all__ = ['SplAtConv2d', 'DropBlock2D']\n",
        "\n",
        "class DropBlock2D(object):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class SplAtConv2d(Module):\n",
        "    \"\"\"Split-Attention Conv2d\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, channels, kernel_size, stride=(1, 1), padding=(0, 0),\n",
        "                 dilation=(1, 1), groups=1, bias=True,\n",
        "                 radix=2, reduction_factor=4,\n",
        "                 rectify=False, rectify_avg=False, norm_layer=None,\n",
        "                 dropblock_prob=0.0, **kwargs):\n",
        "        super(SplAtConv2d, self).__init__()\n",
        "        padding = _pair(padding)\n",
        "        self.rectify = rectify and (padding[0] > 0 or padding[1] > 0)\n",
        "        self.rectify_avg = rectify_avg\n",
        "        inter_channels = max(in_channels*radix//reduction_factor, 32)\n",
        "        self.radix = radix\n",
        "        self.cardinality = groups\n",
        "        self.channels = channels\n",
        "        self.dropblock_prob = dropblock_prob\n",
        "        # if self.rectify:\n",
        "        #     from rfconv import RFConv2d\n",
        "        #     self.conv = RFConv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n",
        "        #                          groups=groups*radix, bias=bias, average_mode=rectify_avg, **kwargs)\n",
        "        # else:\n",
        "        \n",
        "        self.conv = Conv2d(in_channels, channels*radix, kernel_size, stride, padding, dilation,\n",
        "                               groups=groups*radix, bias=bias, **kwargs)\n",
        "        self.use_bn = norm_layer is not None\n",
        "        if self.use_bn:\n",
        "            self.bn0 = norm_layer(channels*radix)\n",
        "        self.relu = ReLU(inplace=True)\n",
        "        self.fc1 = Conv2d(channels, inter_channels, 1, groups=self.cardinality)\n",
        "        if self.use_bn:\n",
        "            self.bn1 = norm_layer(inter_channels)\n",
        "        self.fc2 = Conv2d(inter_channels, channels*radix, 1, groups=self.cardinality)\n",
        "        # if dropblock_prob > 0.0:\n",
        "        #     self.dropblock = DropBlock2D(dropblock_prob, 3)\n",
        "        self.rsoftmax = rSoftMax(radix, groups)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.use_bn:\n",
        "            x = self.bn0(x)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            x = self.dropblock(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        batch, rchannel = x.shape[:2]\n",
        "        if self.radix > 1:\n",
        "            if torch.__version__ < '1.5':\n",
        "                splited = torch.split(x, int(rchannel//self.radix), dim=1)\n",
        "            else:\n",
        "                splited = torch.split(x, rchannel//self.radix, dim=1)\n",
        "            gap = sum(splited) \n",
        "        else:\n",
        "            gap = x\n",
        "        gap = F.adaptive_avg_pool2d(gap, 1)\n",
        "        gap = self.fc1(gap)\n",
        "\n",
        "        if self.use_bn:\n",
        "            gap = self.bn1(gap)\n",
        "        gap = self.relu(gap)\n",
        "\n",
        "        atten = self.fc2(gap)\n",
        "        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n",
        "\n",
        "        if self.radix > 1:\n",
        "            if torch.__version__ < '1.5':\n",
        "                attens = torch.split(atten, int(rchannel//self.radix), dim=1)\n",
        "            else:\n",
        "                attens = torch.split(atten, rchannel//self.radix, dim=1)\n",
        "            out = sum([att*split for (att, split) in zip(attens, splited)])\n",
        "        else:\n",
        "            out = atten * x\n",
        "        return out.contiguous()\n",
        "\n",
        "class rSoftMax(nn.Module):\n",
        "    def __init__(self, radix, cardinality):\n",
        "        super().__init__()\n",
        "        self.radix = radix\n",
        "        self.cardinality = cardinality\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch = x.size(0)\n",
        "        if self.radix > 1:\n",
        "            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n",
        "            x = F.softmax(x, dim=1)\n",
        "            x = x.reshape(batch, -1)\n",
        "        else:\n",
        "            x = torch.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9c6PTUpdIGQ"
      },
      "source": [
        "## BasicBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x9047bJq49I"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"ResNet BasicBlock\n",
        "    \"\"\"\n",
        "    # pylint: disable=unused-argument\n",
        "    expansion = 1\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
        "                 radix=1, cardinality=1, bottleneck_width=64,\n",
        "                 avd=False, avd_first=False, dilation=1, is_first=False,\n",
        "                 rectified_conv=False, rectify_avg=False,\n",
        "                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n",
        "        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = norm_layer(group_width)\n",
        "        self.dropblock_prob = dropblock_prob\n",
        "        self.radix = radix\n",
        "        self.avd = avd and (stride > 1 or is_first)\n",
        "        self.avd_first = avd_first\n",
        "\n",
        "        if self.avd:\n",
        "            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n",
        "            stride = 1\n",
        "\n",
        "        if dropblock_prob > 0.0:\n",
        "            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n",
        "            if radix == 1:\n",
        "                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n",
        "            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n",
        "\n",
        "        if radix >= 1:\n",
        "            self.conv2 = SplAtConv2d(\n",
        "                group_width, group_width, kernel_size=3,\n",
        "                stride=stride, padding=dilation,\n",
        "                dilation=dilation, groups=cardinality, bias=False,\n",
        "                radix=radix, rectify=rectified_conv,\n",
        "                rectify_avg=rectify_avg,\n",
        "                norm_layer=norm_layer,\n",
        "                dropblock_prob=dropblock_prob)\n",
        "        elif rectified_conv:\n",
        "            from rfconv import RFConv2d\n",
        "            self.conv2 = RFConv2d(\n",
        "                group_width, group_width, kernel_size=3, stride=stride,\n",
        "                padding=dilation, dilation=dilation,\n",
        "                groups=cardinality, bias=False,\n",
        "                average_mode=rectify_avg)\n",
        "            self.bn2 = norm_layer(group_width)\n",
        "        else:\n",
        "            self.conv2 = nn.Conv2d(\n",
        "                group_width, group_width, kernel_size=3, stride=stride,\n",
        "                padding=dilation, dilation=dilation,\n",
        "                groups=cardinality, bias=False)\n",
        "            self.bn2 = norm_layer(group_width)\n",
        "\n",
        "        # self.conv3 = nn.Conv2d(\n",
        "        #     group_width, planes * 4, kernel_size=1, bias=False)\n",
        "        # self.bn3 = norm_layer(planes*4)\n",
        "\n",
        "        if last_gamma:\n",
        "            from torch.nn.init import zeros_\n",
        "            zeros_(self.bn3.weight)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.dilation = dilation\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            out = self.dropblock1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        if self.avd and self.avd_first:\n",
        "            out = self.avd_layer(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        if self.radix == 0:\n",
        "            out = self.bn2(out)\n",
        "            if self.dropblock_prob > 0.0:\n",
        "                out = self.dropblock2(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "        if self.avd and not self.avd_first:\n",
        "            out = self.avd_layer(out)\n",
        "\n",
        "        # out = self.conv3(out)\n",
        "        # out = self.bn3(out)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            out = self.dropblock3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEs1RdX0dMOB"
      },
      "source": [
        "## Bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUuQdRt4WEoY"
      },
      "source": [
        "class GlobalAvgPool2d(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n",
        "        super(GlobalAvgPool2d, self).__init__()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return nn.functional.adaptive_avg_pool2d(inputs, 1).view(inputs.size(0), -1)\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"ResNet Bottleneck\n",
        "    \"\"\"\n",
        "    # pylint: disable=unused-argument\n",
        "    expansion = 4\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
        "                 radix=1, cardinality=1, bottleneck_width=64,\n",
        "                 avd=False, avd_first=False, dilation=1, is_first=False,\n",
        "                 rectified_conv=False, rectify_avg=False,\n",
        "                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n",
        "        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n",
        "        self.bn1 = norm_layer(group_width)\n",
        "        self.dropblock_prob = dropblock_prob\n",
        "        self.radix = radix\n",
        "        self.avd = avd and (stride > 1 or is_first)\n",
        "        self.avd_first = avd_first\n",
        "\n",
        "        if self.avd:\n",
        "            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n",
        "            stride = 1\n",
        "\n",
        "        if dropblock_prob > 0.0:\n",
        "            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n",
        "            if radix == 1:\n",
        "                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n",
        "            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n",
        "\n",
        "        if radix >= 1:\n",
        "            self.conv2 = SplAtConv2d(\n",
        "                group_width, group_width, kernel_size=3,\n",
        "                stride=stride, padding=dilation,\n",
        "                dilation=dilation, groups=cardinality, bias=False,\n",
        "                radix=radix, rectify=rectified_conv,\n",
        "                rectify_avg=rectify_avg,\n",
        "                norm_layer=norm_layer,\n",
        "                dropblock_prob=dropblock_prob)\n",
        "        elif rectified_conv:\n",
        "            from rfconv import RFConv2d\n",
        "            self.conv2 = RFConv2d(\n",
        "                group_width, group_width, kernel_size=3, stride=stride,\n",
        "                padding=dilation, dilation=dilation,\n",
        "                groups=cardinality, bias=False,\n",
        "                average_mode=rectify_avg)\n",
        "            self.bn2 = norm_layer(group_width)\n",
        "        else:\n",
        "            self.conv2 = nn.Conv2d(\n",
        "                group_width, group_width, kernel_size=3, stride=stride,\n",
        "                padding=dilation, dilation=dilation,\n",
        "                groups=cardinality, bias=False)\n",
        "            self.bn2 = norm_layer(group_width)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            group_width, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm_layer(planes*4)\n",
        "\n",
        "        if last_gamma:\n",
        "            from torch.nn.init import zeros_\n",
        "            zeros_(self.bn3.weight)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.dilation = dilation\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            out = self.dropblock1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        if self.avd and self.avd_first:\n",
        "            out = self.avd_layer(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        if self.radix == 0:\n",
        "            out = self.bn2(out)\n",
        "            if self.dropblock_prob > 0.0:\n",
        "                out = self.dropblock2(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "        if self.avd and not self.avd_first:\n",
        "            out = self.avd_layer(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            out = self.dropblock3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFrqOGUFdPtp"
      },
      "source": [
        "## ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o96B2sIzcirp"
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    \"\"\"ResNet Variants\n",
        "    Parameters\n",
        "    ----------\n",
        "    block : Block\n",
        "        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n",
        "    layers : list of int\n",
        "        Numbers of layers in each block\n",
        "    classes : int, default 1000\n",
        "        Number of classification classes.\n",
        "    dilated : bool, default False\n",
        "        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n",
        "        typically used in Semantic Segmentation.\n",
        "    norm_layer : object\n",
        "        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n",
        "        for Synchronized Cross-GPU BachNormalization).\n",
        "    Reference:\n",
        "        - He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
        "        - Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\"\n",
        "    \"\"\"\n",
        "    # pylint: disable=unused-variable\n",
        "    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64,\n",
        "                 num_classes=1000, dilated=False, dilation=1,\n",
        "                 deep_stem=False, stem_width=64, avg_down=False,\n",
        "                 rectified_conv=False, rectify_avg=False,\n",
        "                 avd=False, avd_first=False,\n",
        "                 final_drop=0.0, dropblock_prob=0,\n",
        "                 last_gamma=False, norm_layer=nn.BatchNorm2d):\n",
        "        self.cardinality = groups\n",
        "        self.bottleneck_width = bottleneck_width\n",
        "        # ResNet-D params\n",
        "        self.inplanes = stem_width*2 if deep_stem else 64\n",
        "        self.avg_down = avg_down\n",
        "        self.last_gamma = last_gamma\n",
        "        # ResNeSt params\n",
        "        self.radix = radix\n",
        "        self.avd = avd\n",
        "        self.avd_first = avd_first\n",
        "\n",
        "        super(ResNet, self).__init__()\n",
        "        self.rectified_conv = rectified_conv\n",
        "        self.rectify_avg = rectify_avg\n",
        "        if rectified_conv:\n",
        "            from rfconv import RFConv2d\n",
        "            conv_layer = RFConv2d\n",
        "        else:\n",
        "            conv_layer = nn.Conv2d\n",
        "        conv_kwargs = {'average_mode': rectify_avg} if rectified_conv else {}\n",
        "        if deep_stem:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                conv_layer(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n",
        "                norm_layer(stem_width),\n",
        "                nn.ReLU(inplace=True),\n",
        "                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
        "                norm_layer(stem_width),\n",
        "                nn.ReLU(inplace=True),\n",
        "                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = conv_layer(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                                   bias=False, **conv_kwargs)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n",
        "        if dilated or dilation == 4:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n",
        "                                           dilation=2, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
        "                                           dilation=4, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        elif dilation==2:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                           dilation=1, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
        "                                           dilation=2, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        else:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                           norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                           norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        self.avgpool = GlobalAvgPool2d()\n",
        "        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, norm_layer):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None,\n",
        "                    dropblock_prob=0.0, is_first=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            down_layers = []\n",
        "            if self.avg_down:\n",
        "                if dilation == 1:\n",
        "                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride,\n",
        "                                                    ceil_mode=True, count_include_pad=False))\n",
        "                else:\n",
        "                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1,\n",
        "                                                    ceil_mode=True, count_include_pad=False))\n",
        "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                             kernel_size=1, stride=1, bias=False))\n",
        "            else:\n",
        "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                             kernel_size=1, stride=stride, bias=False))\n",
        "            down_layers.append(norm_layer(planes * block.expansion))\n",
        "            downsample = nn.Sequential(*down_layers)\n",
        "\n",
        "        layers = []\n",
        "        if dilation == 1 or dilation == 2:\n",
        "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=1, is_first=is_first, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "        elif dilation == 4:\n",
        "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=2, is_first=is_first, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "        else:\n",
        "            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=dilation, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        if self.drop:\n",
        "            x = self.drop(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8QU46yidElr"
      },
      "source": [
        "def resnet50(pretrained=False, root='~/.encoding/models', **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(torch.hub.load_state_dict_from_url(\n",
        "            resnest_model_urls['resnet50'], progress=True, check_hash=True))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtPvb-sQ0p7C"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aAz1pal1de9"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPa8oeFL1eAi"
      },
      "source": [
        "# cd drive/MyDrive/Colab\\ Notebooks/3033_HW4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukcDSfQWW1CP"
      },
      "source": [
        "dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TmGKFn01gzo"
      },
      "source": [
        "data_dir = 'data/cifar10'\n",
        "batch_size = 128\n",
        "\n",
        "def get_data_loaders(data_dir,\n",
        "                     batch_size,\n",
        "                     train_transform,\n",
        "                     test_transform,\n",
        "                     shuffle=True,\n",
        "                     num_workers=4,\n",
        "                     pin_memory=False):\n",
        "    \"\"\"\n",
        "    Adapted from: https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb\n",
        "    \n",
        "    Utility function for loading and returning train and test\n",
        "    multi-process iterators over the CIFAR-10 dataset.\n",
        "    If using CUDA, set pin_memory to True.\n",
        "    \n",
        "    Params\n",
        "    ------\n",
        "    - data_dir: path directory to the dataset.\n",
        "    - batch_size: how many samples per batch to load.\n",
        "    - train_transform: pytorch transforms for the training set\n",
        "    - test_transform: pytorch transofrms for the test set\n",
        "    - num_workers: number of subprocesses to use when loading the dataset.\n",
        "    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n",
        "      True if using GPU.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    - train_loader: training set iterator.\n",
        "    - test_loader:  test set iterator.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load the datasets\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=train_transform,\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=False,\n",
        "        download=True, transform=test_transform,\n",
        "    )\n",
        "    \n",
        "    # Create loader objects\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "        num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "        num_workers=num_workers, pin_memory=pin_memory\n",
        "    )\n",
        "          \n",
        "    return (train_loader, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaFZHH_SW9fq"
      },
      "source": [
        "train & evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au2kTS3MWzt9"
      },
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Calculate classification error (%) for given model\n",
        "    and data set.\n",
        "    \n",
        "    Parameters:\n",
        "    \n",
        "    - model: A Trained Pytorch Model \n",
        "    - data_loader: A Pytorch data loader object\n",
        "    \"\"\"\n",
        "    \n",
        "    y_true = np.array([], dtype=np.int)\n",
        "    y_pred = np.array([], dtype=np.int)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            \n",
        "            y_true = np.concatenate((y_true, labels.cpu()))\n",
        "            y_pred = np.concatenate((y_pred, predicted.cpu()))\n",
        "    \n",
        "    error = np.sum(y_pred != y_true) / len(y_true)\n",
        "    return error\n",
        "\n",
        "\n",
        "\n",
        "def train(model, epochs, train_loader, test_loader, criterion, \n",
        "          optimizer, RESULTS_PATH, scheduler=None, MODEL_PATH=None):\n",
        "    \"\"\"\n",
        "    End to end training as described by the original resnet paper:\n",
        "    https://arxiv.org/abs/1512.03385\n",
        "    \n",
        "    Parameters\n",
        "    ----------------\n",
        "    \n",
        "    - model: The PyTorch model to be trained\n",
        "    - n:   Determines depth of the neural network \n",
        "           as described in paper\n",
        "    - train_loader: \n",
        "           PyTorch dataloader object for training set\n",
        "    - test_loader: \n",
        "           PyTorch dataloader object for test set\n",
        "    \"\"\"\n",
        "    \n",
        "    # Run on GPU if available\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "    model.to(device)\n",
        "    \n",
        "    # Training loop\n",
        "    # -------------------------------\n",
        "    cols       = ['epoch', 'train_loss', 'train_acc', 'test_acc', 'time']\n",
        "    results_df = pd.DataFrame(columns=cols).set_index('epoch')\n",
        "    # print('Epoch \\tBatch \\tNLLLoss_Train')\n",
        "    \n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        running_loss  = 0.0\n",
        "        best_test_err = 1.0\n",
        "        for i, data in enumerate(train_loader, 0):   # Do a batch iteration\n",
        "            \n",
        "            # get the inputs\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # # print average loss for last 50 mini-batches\n",
        "            # running_loss += loss.item()\n",
        "            # if i % 50 == 49:\n",
        "            #     print('%d \\t%d \\t%.3f' %\n",
        "            #           (epoch + 1, i + 1, running_loss / 50))\n",
        "            #     running_loss = 0.0\n",
        "        \n",
        "        end = time.time()\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        \n",
        "        # Record metrics\n",
        "        model.eval()\n",
        "        train_loss = np.round(loss.item(), 4)\n",
        "        train_err = evaluate(model, train_loader, device)\n",
        "        test_err = evaluate(model, test_loader, device)\n",
        "        train_acc = np.round(1 - train_err, 4)\n",
        "        test_acc = np.round(1 - test_err, 4)\n",
        "        results_df.loc[epoch] = [train_loss, train_acc, test_acc, end - start]\n",
        "        results_df.to_csv(RESULTS_PATH)\n",
        "        print(f'epoch: {epoch} training loss: {train_loss} train_acc: {train_acc} test_acc: {test_acc} time: {end-start}')\n",
        "        \n",
        "        # Save best model\n",
        "        if MODEL_PATH and (test_err < best_test_err):\n",
        "            torch.save(model.state_dict(), MODEL_PATH)\n",
        "            best_test_err = test_err\n",
        "        \n",
        "    print('Finished Training')\n",
        "    model.eval()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZSW7Q5kXfU8"
      },
      "source": [
        "data transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWwgfjKRXV3k"
      },
      "source": [
        "# Normalisation parameters fo CIFAR10\n",
        "means = [0.4918687901200927, 0.49185976472299225, 0.4918583862227116]\n",
        "stds  = [0.24697121702736, 0.24696766978537033, 0.2469719877121087]\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "    mean=means,\n",
        "    std=stds,\n",
        ")\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # 4 pixels are padded on each side, \n",
        "    transforms.Pad(4),\n",
        "    # a 32×32 crop is randomly sampled from the \n",
        "    # padded image or its horizontal flip.\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    # For testing, we only evaluate the single \n",
        "    # view of the original 32×32 image.\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6of1w1BvZ8Oq",
        "outputId": "a0f6c2dc-44a8-4fe4-baec-05c69e7a4df7"
      },
      "source": [
        "train_loader, test_loader = get_data_loaders(data_dir,\n",
        "                                              batch_size,\n",
        "                                              train_transform,\n",
        "                                              test_transform,\n",
        "                                              shuffle=True,\n",
        "                                              num_workers=4,\n",
        "                                              pin_memory=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raODtq8KXqWr"
      },
      "source": [
        "epochs = 250\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "weight_decay = 0.0001 \n",
        "milestones = [80,210]\n",
        "gamma = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emB6MsETapaj"
      },
      "source": [
        "### Basic Block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M19Di6RqasGT",
        "outputId": "3ccd7026-38af-4005-e4df-1868dcc67b0f"
      },
      "source": [
        "model_basic = ResNet(BasicBlock, [3, 4, 14, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,num_classes=10,\n",
        "                   deep_stem=True, stem_width=32, avg_down=True,\n",
        "                   avd=True, avd_first=False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_basic.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "results_file = f'resnest50_basic.csv'\n",
        "model_file = f'resnest50_basic.pt'\n",
        "train(model_basic, epochs, train_loader, test_loader, criterion, \n",
        "      optimizer, results_file, scheduler=scheduler, MODEL_PATH=model_file )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "epoch: 0 |train_loss: 2.3386178016662598 | train_acc: 0.10402 | test_acc: 0.1066\n",
            "epoch: 1 |train_loss: 2.0878043174743652 | train_acc: 0.20702 | test_acc: 0.215\n",
            "epoch: 2 |train_loss: 1.7789520025253296 | train_acc: 0.27354 | test_acc: 0.286\n",
            "epoch: 3 |train_loss: 1.779820203781128 | train_acc: 0.3192 | test_acc: 0.3198\n",
            "epoch: 4 |train_loss: 1.7175089120864868 | train_acc: 0.3558 | test_acc: 0.3684\n",
            "epoch: 5 |train_loss: 1.4531006813049316 | train_acc: 0.33676 | test_acc: 0.3578\n",
            "epoch: 6 |train_loss: 1.6070306301116943 | train_acc: 0.42198 | test_acc: 0.4282\n",
            "epoch: 7 |train_loss: 1.3667398691177368 | train_acc: 0.42292 | test_acc: 0.4384\n",
            "epoch: 8 |train_loss: 1.5610198974609375 | train_acc: 0.31044 | test_acc: 0.3393\n",
            "epoch: 9 |train_loss: 1.6339101791381836 | train_acc: 0.3239 | test_acc: 0.3474\n",
            "epoch: 10 |train_loss: 1.4388227462768555 | train_acc: 0.45424 | test_acc: 0.4649\n",
            "epoch: 11 |train_loss: 1.4744917154312134 | train_acc: 0.45604 | test_acc: 0.4629\n",
            "epoch: 12 |train_loss: 1.295396089553833 | train_acc: 0.44086 | test_acc: 0.4555\n",
            "epoch: 13 |train_loss: 1.349422812461853 | train_acc: 0.52568 | test_acc: 0.5273\n",
            "epoch: 14 |train_loss: 1.5837559700012207 | train_acc: 0.50402 | test_acc: 0.5145\n",
            "epoch: 15 |train_loss: 1.434275507926941 | train_acc: 0.4253 | test_acc: 0.4039\n",
            "epoch: 16 |train_loss: 1.3345171213150024 | train_acc: 0.52748 | test_acc: 0.5274\n",
            "epoch: 17 |train_loss: 1.086118221282959 | train_acc: 0.5279 | test_acc: 0.5457\n",
            "epoch: 18 |train_loss: 1.6042652130126953 | train_acc: 0.52018 | test_acc: 0.5337\n",
            "epoch: 19 |train_loss: 1.2329840660095215 | train_acc: 0.59394 | test_acc: 0.6051\n",
            "epoch: 20 |train_loss: 1.1478469371795654 | train_acc: 0.61562 | test_acc: 0.609\n",
            "epoch: 21 |train_loss: 1.1575686931610107 | train_acc: 0.61854 | test_acc: 0.6213\n",
            "epoch: 22 |train_loss: 0.9882687330245972 | train_acc: 0.63028 | test_acc: 0.6356\n",
            "epoch: 23 |train_loss: 0.993404746055603 | train_acc: 0.6106 | test_acc: 0.6268\n",
            "epoch: 24 |train_loss: 0.8192126154899597 | train_acc: 0.70058 | test_acc: 0.7049\n",
            "epoch: 25 |train_loss: 0.7958807945251465 | train_acc: 0.7029 | test_acc: 0.7005\n",
            "epoch: 26 |train_loss: 0.7386716604232788 | train_acc: 0.68632 | test_acc: 0.6722\n",
            "epoch: 27 |train_loss: 0.8572235107421875 | train_acc: 0.72962 | test_acc: 0.7179\n",
            "epoch: 28 |train_loss: 0.7319550514221191 | train_acc: 0.6887 | test_acc: 0.6918\n",
            "epoch: 29 |train_loss: 0.5450826287269592 | train_acc: 0.73562 | test_acc: 0.7168\n",
            "epoch: 30 |train_loss: 0.5880222320556641 | train_acc: 0.73594 | test_acc: 0.7375\n",
            "epoch: 31 |train_loss: 0.7502380013465881 | train_acc: 0.73404 | test_acc: 0.7229\n",
            "epoch: 32 |train_loss: 0.7752161622047424 | train_acc: 0.779 | test_acc: 0.7651\n",
            "epoch: 33 |train_loss: 0.6036539673805237 | train_acc: 0.78582 | test_acc: 0.7666\n",
            "epoch: 34 |train_loss: 0.7853518724441528 | train_acc: 0.75494 | test_acc: 0.7351\n",
            "epoch: 35 |train_loss: 0.5968708992004395 | train_acc: 0.79454 | test_acc: 0.7771\n",
            "epoch: 36 |train_loss: 0.8655326962471008 | train_acc: 0.71276 | test_acc: 0.7038\n",
            "epoch: 37 |train_loss: 0.6850012540817261 | train_acc: 0.78906 | test_acc: 0.7693\n",
            "epoch: 38 |train_loss: 0.7005230188369751 | train_acc: 0.79442 | test_acc: 0.7761\n",
            "epoch: 39 |train_loss: 0.4528624415397644 | train_acc: 0.79144 | test_acc: 0.7682\n",
            "epoch: 40 |train_loss: 0.7558532953262329 | train_acc: 0.75282 | test_acc: 0.7201\n",
            "epoch: 41 |train_loss: 0.4452420771121979 | train_acc: 0.80962 | test_acc: 0.7865\n",
            "epoch: 42 |train_loss: 0.32438144087791443 | train_acc: 0.81626 | test_acc: 0.7989\n",
            "epoch: 43 |train_loss: 0.5785723924636841 | train_acc: 0.82632 | test_acc: 0.7948\n",
            "epoch: 44 |train_loss: 0.43670862913131714 | train_acc: 0.81416 | test_acc: 0.7868\n",
            "epoch: 45 |train_loss: 0.60179203748703 | train_acc: 0.79656 | test_acc: 0.765\n",
            "epoch: 46 |train_loss: 0.5275915861129761 | train_acc: 0.81724 | test_acc: 0.7727\n",
            "epoch: 47 |train_loss: 0.4168315529823303 | train_acc: 0.82676 | test_acc: 0.7926\n",
            "epoch: 48 |train_loss: 0.3425949215888977 | train_acc: 0.78742 | test_acc: 0.7477\n",
            "epoch: 49 |train_loss: 0.4367341995239258 | train_acc: 0.78954 | test_acc: 0.753\n",
            "epoch: 50 |train_loss: 0.5068039894104004 | train_acc: 0.84936 | test_acc: 0.8115\n",
            "epoch: 51 |train_loss: 0.4687533378601074 | train_acc: 0.81686 | test_acc: 0.7789\n",
            "epoch: 52 |train_loss: 0.618311882019043 | train_acc: 0.81814 | test_acc: 0.7917\n",
            "epoch: 53 |train_loss: 0.4208117425441742 | train_acc: 0.84798 | test_acc: 0.8211\n",
            "epoch: 54 |train_loss: 0.2595391571521759 | train_acc: 0.851 | test_acc: 0.8182\n",
            "epoch: 55 |train_loss: 0.4155272841453552 | train_acc: 0.86206 | test_acc: 0.8242\n",
            "epoch: 56 |train_loss: 0.48699960112571716 | train_acc: 0.79968 | test_acc: 0.7682\n",
            "epoch: 57 |train_loss: 0.36113467812538147 | train_acc: 0.84528 | test_acc: 0.8138\n",
            "epoch: 58 |train_loss: 0.39683976769447327 | train_acc: 0.85688 | test_acc: 0.8247\n",
            "epoch: 59 |train_loss: 0.43692249059677124 | train_acc: 0.80952 | test_acc: 0.7752\n",
            "epoch: 60 |train_loss: 0.6384625434875488 | train_acc: 0.81892 | test_acc: 0.7816\n",
            "epoch: 61 |train_loss: 0.35928502678871155 | train_acc: 0.83634 | test_acc: 0.8037\n",
            "epoch: 62 |train_loss: 0.17379668354988098 | train_acc: 0.83948 | test_acc: 0.7984\n",
            "epoch: 63 |train_loss: 0.38794103264808655 | train_acc: 0.85426 | test_acc: 0.8116\n",
            "epoch: 64 |train_loss: 0.27837857604026794 | train_acc: 0.82156 | test_acc: 0.785\n",
            "epoch: 65 |train_loss: 0.3604617416858673 | train_acc: 0.87202 | test_acc: 0.8281\n",
            "epoch: 66 |train_loss: 0.5239497423171997 | train_acc: 0.85452 | test_acc: 0.8125\n",
            "epoch: 67 |train_loss: 0.45343223214149475 | train_acc: 0.84888 | test_acc: 0.8053\n",
            "epoch: 68 |train_loss: 0.40901440382003784 | train_acc: 0.83454 | test_acc: 0.7853\n",
            "epoch: 69 |train_loss: 0.2693490982055664 | train_acc: 0.85282 | test_acc: 0.8136\n",
            "epoch: 70 |train_loss: 0.5294139385223389 | train_acc: 0.85234 | test_acc: 0.8076\n",
            "epoch: 71 |train_loss: 0.3650016486644745 | train_acc: 0.85556 | test_acc: 0.8042\n",
            "epoch: 72 |train_loss: 0.4418913424015045 | train_acc: 0.88384 | test_acc: 0.8416\n",
            "epoch: 73 |train_loss: 0.46998223662376404 | train_acc: 0.8466 | test_acc: 0.8011\n",
            "epoch: 74 |train_loss: 0.33503034710884094 | train_acc: 0.88884 | test_acc: 0.8391\n",
            "epoch: 75 |train_loss: 0.46320080757141113 | train_acc: 0.89282 | test_acc: 0.8455\n",
            "epoch: 76 |train_loss: 0.48834753036499023 | train_acc: 0.87824 | test_acc: 0.8336\n",
            "epoch: 77 |train_loss: 0.3156975209712982 | train_acc: 0.86424 | test_acc: 0.817\n",
            "epoch: 78 |train_loss: 0.27183932065963745 | train_acc: 0.87232 | test_acc: 0.8201\n",
            "epoch: 79 |train_loss: 0.2892226576805115 | train_acc: 0.8445 | test_acc: 0.8048\n",
            "epoch: 80 |train_loss: 0.12488076835870743 | train_acc: 0.94774 | test_acc: 0.881\n",
            "epoch: 81 |train_loss: 0.20080943405628204 | train_acc: 0.95324 | test_acc: 0.8841\n",
            "epoch: 82 |train_loss: 0.1669822335243225 | train_acc: 0.95718 | test_acc: 0.8847\n",
            "epoch: 83 |train_loss: 0.08707605302333832 | train_acc: 0.96028 | test_acc: 0.8845\n",
            "epoch: 84 |train_loss: 0.0898008793592453 | train_acc: 0.9636 | test_acc: 0.8871\n",
            "epoch: 85 |train_loss: 0.17384693026542664 | train_acc: 0.96514 | test_acc: 0.8859\n",
            "epoch: 86 |train_loss: 0.23120789229869843 | train_acc: 0.96712 | test_acc: 0.8859\n",
            "epoch: 87 |train_loss: 0.14125069975852966 | train_acc: 0.96832 | test_acc: 0.886\n",
            "epoch: 88 |train_loss: 0.2654021382331848 | train_acc: 0.9709 | test_acc: 0.8843\n",
            "epoch: 89 |train_loss: 0.08412183821201324 | train_acc: 0.97146 | test_acc: 0.8843\n",
            "epoch: 90 |train_loss: 0.14186540246009827 | train_acc: 0.97214 | test_acc: 0.887\n",
            "epoch: 91 |train_loss: 0.07026845216751099 | train_acc: 0.97318 | test_acc: 0.8852\n",
            "epoch: 92 |train_loss: 0.03841085731983185 | train_acc: 0.97476 | test_acc: 0.8803\n",
            "epoch: 93 |train_loss: 0.04866791516542435 | train_acc: 0.9754 | test_acc: 0.8849\n",
            "epoch: 94 |train_loss: 0.17321093380451202 | train_acc: 0.97724 | test_acc: 0.8869\n",
            "epoch: 95 |train_loss: 0.10035469383001328 | train_acc: 0.9766 | test_acc: 0.8829\n",
            "epoch: 96 |train_loss: 0.03690357506275177 | train_acc: 0.9772 | test_acc: 0.8865\n",
            "epoch: 97 |train_loss: 0.09842155873775482 | train_acc: 0.97792 | test_acc: 0.8847\n",
            "epoch: 98 |train_loss: 0.023419294506311417 | train_acc: 0.9792 | test_acc: 0.8854\n",
            "epoch: 99 |train_loss: 0.07907631248235703 | train_acc: 0.98092 | test_acc: 0.8852\n",
            "epoch: 100 |train_loss: 0.029228636994957924 | train_acc: 0.98136 | test_acc: 0.8852\n",
            "epoch: 101 |train_loss: 0.11515500396490097 | train_acc: 0.98136 | test_acc: 0.8871\n",
            "epoch: 102 |train_loss: 0.07765564322471619 | train_acc: 0.98196 | test_acc: 0.8862\n",
            "epoch: 103 |train_loss: 0.038506537675857544 | train_acc: 0.98192 | test_acc: 0.8804\n",
            "epoch: 104 |train_loss: 0.047257307916879654 | train_acc: 0.9823 | test_acc: 0.8834\n",
            "epoch: 105 |train_loss: 0.03799434006214142 | train_acc: 0.98228 | test_acc: 0.8824\n",
            "epoch: 106 |train_loss: 0.027730826288461685 | train_acc: 0.98472 | test_acc: 0.8866\n",
            "epoch: 107 |train_loss: 0.0923476591706276 | train_acc: 0.98328 | test_acc: 0.8842\n",
            "epoch: 108 |train_loss: 0.046969424933195114 | train_acc: 0.98456 | test_acc: 0.885\n",
            "epoch: 109 |train_loss: 0.07889819890260696 | train_acc: 0.98516 | test_acc: 0.8857\n",
            "epoch: 110 |train_loss: 0.05526730418205261 | train_acc: 0.98502 | test_acc: 0.8827\n",
            "epoch: 111 |train_loss: 0.049046266824007034 | train_acc: 0.98522 | test_acc: 0.8838\n",
            "epoch: 112 |train_loss: 0.08080290257930756 | train_acc: 0.98646 | test_acc: 0.8842\n",
            "epoch: 113 |train_loss: 0.08462303131818771 | train_acc: 0.98638 | test_acc: 0.884\n",
            "epoch: 114 |train_loss: 0.08695200085639954 | train_acc: 0.98506 | test_acc: 0.8834\n",
            "epoch: 115 |train_loss: 0.11204258352518082 | train_acc: 0.98708 | test_acc: 0.8835\n",
            "epoch: 116 |train_loss: 0.15723370015621185 | train_acc: 0.98696 | test_acc: 0.8791\n",
            "epoch: 117 |train_loss: 0.17179249227046967 | train_acc: 0.98608 | test_acc: 0.8845\n",
            "epoch: 118 |train_loss: 0.05364441126585007 | train_acc: 0.98822 | test_acc: 0.8832\n",
            "epoch: 119 |train_loss: 0.11310245841741562 | train_acc: 0.98858 | test_acc: 0.8833\n",
            "epoch: 120 |train_loss: 0.07572124898433685 | train_acc: 0.988 | test_acc: 0.8846\n",
            "epoch: 121 |train_loss: 0.029574623331427574 | train_acc: 0.9861 | test_acc: 0.8827\n",
            "epoch: 122 |train_loss: 0.0633956715464592 | train_acc: 0.98862 | test_acc: 0.8837\n",
            "epoch: 123 |train_loss: 0.0189969539642334 | train_acc: 0.9894 | test_acc: 0.8833\n",
            "epoch: 124 |train_loss: 0.049609631299972534 | train_acc: 0.98806 | test_acc: 0.8826\n",
            "epoch: 125 |train_loss: 0.02712138369679451 | train_acc: 0.98774 | test_acc: 0.884\n",
            "epoch: 126 |train_loss: 0.030435284599661827 | train_acc: 0.98956 | test_acc: 0.8843\n",
            "epoch: 127 |train_loss: 0.013177162036299706 | train_acc: 0.98702 | test_acc: 0.8827\n",
            "epoch: 128 |train_loss: 0.07206748425960541 | train_acc: 0.98844 | test_acc: 0.884\n",
            "epoch: 129 |train_loss: 0.02936697006225586 | train_acc: 0.98924 | test_acc: 0.8854\n",
            "epoch: 130 |train_loss: 0.08536743372678757 | train_acc: 0.98872 | test_acc: 0.8834\n",
            "epoch: 131 |train_loss: 0.05987905338406563 | train_acc: 0.98914 | test_acc: 0.8841\n",
            "epoch: 132 |train_loss: 0.07012676447629929 | train_acc: 0.99028 | test_acc: 0.888\n",
            "epoch: 133 |train_loss: 0.12786166369915009 | train_acc: 0.98844 | test_acc: 0.882\n",
            "epoch: 134 |train_loss: 0.05784596875309944 | train_acc: 0.98914 | test_acc: 0.8836\n",
            "epoch: 135 |train_loss: 0.019274035468697548 | train_acc: 0.98892 | test_acc: 0.8829\n",
            "epoch: 136 |train_loss: 0.008656861260533333 | train_acc: 0.98908 | test_acc: 0.8796\n",
            "epoch: 137 |train_loss: 0.004269033204764128 | train_acc: 0.99032 | test_acc: 0.883\n",
            "epoch: 138 |train_loss: 0.010524971410632133 | train_acc: 0.99066 | test_acc: 0.8818\n",
            "epoch: 139 |train_loss: 0.05105621740221977 | train_acc: 0.98942 | test_acc: 0.8838\n",
            "epoch: 140 |train_loss: 0.0215250626206398 | train_acc: 0.9901 | test_acc: 0.8822\n",
            "epoch: 141 |train_loss: 0.03134501352906227 | train_acc: 0.98976 | test_acc: 0.8823\n",
            "epoch: 142 |train_loss: 0.004713807255029678 | train_acc: 0.98978 | test_acc: 0.882\n",
            "epoch: 143 |train_loss: 0.07901225984096527 | train_acc: 0.99022 | test_acc: 0.882\n",
            "epoch: 144 |train_loss: 0.05467650294303894 | train_acc: 0.99062 | test_acc: 0.8826\n",
            "epoch: 145 |train_loss: 0.008632891811430454 | train_acc: 0.99082 | test_acc: 0.8841\n",
            "epoch: 146 |train_loss: 0.07272852957248688 | train_acc: 0.9889 | test_acc: 0.8857\n",
            "epoch: 147 |train_loss: 0.030147427693009377 | train_acc: 0.99064 | test_acc: 0.8834\n",
            "epoch: 148 |train_loss: 0.024081673473119736 | train_acc: 0.9895 | test_acc: 0.8839\n",
            "epoch: 149 |train_loss: 0.07370366901159286 | train_acc: 0.99066 | test_acc: 0.8813\n",
            "epoch: 150 |train_loss: 0.056678980588912964 | train_acc: 0.9895 | test_acc: 0.8814\n",
            "epoch: 151 |train_loss: 0.006951450370252132 | train_acc: 0.9907 | test_acc: 0.8867\n",
            "epoch: 152 |train_loss: 0.06556424498558044 | train_acc: 0.9887 | test_acc: 0.8788\n",
            "epoch: 153 |train_loss: 0.058745186775922775 | train_acc: 0.98978 | test_acc: 0.8816\n",
            "epoch: 154 |train_loss: 0.01252389419823885 | train_acc: 0.99044 | test_acc: 0.8802\n",
            "epoch: 155 |train_loss: 0.03470882028341293 | train_acc: 0.9893 | test_acc: 0.882\n",
            "epoch: 156 |train_loss: 0.014126415364444256 | train_acc: 0.98988 | test_acc: 0.8828\n",
            "epoch: 157 |train_loss: 0.03368600457906723 | train_acc: 0.98908 | test_acc: 0.8818\n",
            "epoch: 158 |train_loss: 0.033974550664424896 | train_acc: 0.99152 | test_acc: 0.8851\n",
            "epoch: 159 |train_loss: 0.013944399543106556 | train_acc: 0.99072 | test_acc: 0.8814\n",
            "epoch: 160 |train_loss: 0.12152791023254395 | train_acc: 0.9913 | test_acc: 0.8838\n",
            "epoch: 161 |train_loss: 0.01273399405181408 | train_acc: 0.99122 | test_acc: 0.8827\n",
            "epoch: 162 |train_loss: 0.012668224051594734 | train_acc: 0.99204 | test_acc: 0.8853\n",
            "epoch: 163 |train_loss: 0.02845175564289093 | train_acc: 0.99086 | test_acc: 0.8858\n",
            "epoch: 164 |train_loss: 0.011879855766892433 | train_acc: 0.98936 | test_acc: 0.8834\n",
            "epoch: 165 |train_loss: 0.05844321846961975 | train_acc: 0.99108 | test_acc: 0.8833\n",
            "epoch: 166 |train_loss: 0.03159252554178238 | train_acc: 0.98978 | test_acc: 0.8826\n",
            "epoch: 167 |train_loss: 0.010538533329963684 | train_acc: 0.9911 | test_acc: 0.8824\n",
            "epoch: 168 |train_loss: 0.04903404787182808 | train_acc: 0.98834 | test_acc: 0.8784\n",
            "epoch: 169 |train_loss: 0.06421246379613876 | train_acc: 0.98864 | test_acc: 0.8824\n",
            "epoch: 170 |train_loss: 0.041700538247823715 | train_acc: 0.98986 | test_acc: 0.88\n",
            "epoch: 171 |train_loss: 0.02990608476102352 | train_acc: 0.98786 | test_acc: 0.8772\n",
            "epoch: 172 |train_loss: 0.018576910719275475 | train_acc: 0.9908 | test_acc: 0.8838\n",
            "epoch: 173 |train_loss: 0.0898434966802597 | train_acc: 0.99068 | test_acc: 0.8805\n",
            "epoch: 174 |train_loss: 0.08018516004085541 | train_acc: 0.98968 | test_acc: 0.8808\n",
            "epoch: 175 |train_loss: 0.015906479209661484 | train_acc: 0.99166 | test_acc: 0.8804\n",
            "epoch: 176 |train_loss: 0.03293001279234886 | train_acc: 0.98938 | test_acc: 0.8805\n",
            "epoch: 177 |train_loss: 0.03161370009183884 | train_acc: 0.98834 | test_acc: 0.8818\n",
            "epoch: 178 |train_loss: 0.01802394911646843 | train_acc: 0.99134 | test_acc: 0.8874\n",
            "epoch: 179 |train_loss: 0.041449688374996185 | train_acc: 0.99094 | test_acc: 0.8825\n",
            "epoch: 180 |train_loss: 0.015649374574422836 | train_acc: 0.99104 | test_acc: 0.8827\n",
            "epoch: 181 |train_loss: 0.03991769626736641 | train_acc: 0.99168 | test_acc: 0.8835\n",
            "epoch: 182 |train_loss: 0.01538581121712923 | train_acc: 0.9909 | test_acc: 0.8797\n",
            "epoch: 183 |train_loss: 0.06900501996278763 | train_acc: 0.99062 | test_acc: 0.8829\n",
            "epoch: 184 |train_loss: 0.08366215229034424 | train_acc: 0.99082 | test_acc: 0.881\n",
            "epoch: 185 |train_loss: 0.030140837654471397 | train_acc: 0.99084 | test_acc: 0.88\n",
            "epoch: 186 |train_loss: 0.06084365397691727 | train_acc: 0.99118 | test_acc: 0.8807\n",
            "epoch: 187 |train_loss: 0.03533206507563591 | train_acc: 0.99094 | test_acc: 0.8829\n",
            "epoch: 188 |train_loss: 0.05429627373814583 | train_acc: 0.99118 | test_acc: 0.8818\n",
            "epoch: 189 |train_loss: 0.06598255038261414 | train_acc: 0.98878 | test_acc: 0.8794\n",
            "epoch: 190 |train_loss: 0.044540900737047195 | train_acc: 0.99086 | test_acc: 0.8841\n",
            "epoch: 191 |train_loss: 0.053980033844709396 | train_acc: 0.98916 | test_acc: 0.8824\n",
            "epoch: 192 |train_loss: 0.023816566914319992 | train_acc: 0.9899 | test_acc: 0.8792\n",
            "epoch: 193 |train_loss: 0.017212999984622 | train_acc: 0.98918 | test_acc: 0.879\n",
            "epoch: 194 |train_loss: 0.021864721551537514 | train_acc: 0.9908 | test_acc: 0.8789\n",
            "epoch: 195 |train_loss: 0.04784541577100754 | train_acc: 0.99046 | test_acc: 0.8813\n",
            "epoch: 196 |train_loss: 0.004510210826992989 | train_acc: 0.99236 | test_acc: 0.8854\n",
            "epoch: 197 |train_loss: 0.012845267541706562 | train_acc: 0.9902 | test_acc: 0.8797\n",
            "epoch: 198 |train_loss: 0.03246770054101944 | train_acc: 0.98986 | test_acc: 0.8816\n",
            "epoch: 199 |train_loss: 0.00962517224252224 | train_acc: 0.98932 | test_acc: 0.8835\n",
            "epoch: 200 |train_loss: 0.01934056170284748 | train_acc: 0.9906 | test_acc: 0.8841\n",
            "epoch: 201 |train_loss: 0.0365738645195961 | train_acc: 0.98968 | test_acc: 0.8828\n",
            "epoch: 202 |train_loss: 0.028850574046373367 | train_acc: 0.99202 | test_acc: 0.8793\n",
            "epoch: 203 |train_loss: 0.031924791634082794 | train_acc: 0.98856 | test_acc: 0.8817\n",
            "epoch: 204 |train_loss: 0.029456187039613724 | train_acc: 0.9885 | test_acc: 0.8796\n",
            "epoch: 205 |train_loss: 0.014437814243137836 | train_acc: 0.99148 | test_acc: 0.8802\n",
            "epoch: 206 |train_loss: 0.017152398824691772 | train_acc: 0.98804 | test_acc: 0.8793\n",
            "epoch: 207 |train_loss: 0.08650065213441849 | train_acc: 0.98992 | test_acc: 0.885\n",
            "epoch: 208 |train_loss: 0.049270667135715485 | train_acc: 0.98826 | test_acc: 0.8785\n",
            "epoch: 209 |train_loss: 0.03216385096311569 | train_acc: 0.99046 | test_acc: 0.8824\n",
            "epoch: 210 |train_loss: 0.05574820190668106 | train_acc: 0.99596 | test_acc: 0.8895\n",
            "epoch: 211 |train_loss: 0.06497515738010406 | train_acc: 0.99686 | test_acc: 0.8903\n",
            "epoch: 212 |train_loss: 0.004713858012109995 | train_acc: 0.9976 | test_acc: 0.8912\n",
            "epoch: 213 |train_loss: 0.009704024530947208 | train_acc: 0.99792 | test_acc: 0.8907\n",
            "epoch: 214 |train_loss: 0.07053227722644806 | train_acc: 0.99788 | test_acc: 0.8912\n",
            "epoch: 215 |train_loss: 0.02397209033370018 | train_acc: 0.9981 | test_acc: 0.8923\n",
            "epoch: 216 |train_loss: 0.003609119448810816 | train_acc: 0.99822 | test_acc: 0.8927\n",
            "epoch: 217 |train_loss: 0.0042229811660945415 | train_acc: 0.99856 | test_acc: 0.8926\n",
            "epoch: 218 |train_loss: 0.011433917097747326 | train_acc: 0.9985 | test_acc: 0.8924\n",
            "epoch: 219 |train_loss: 0.009691563434898853 | train_acc: 0.99878 | test_acc: 0.8937\n",
            "epoch: 220 |train_loss: 0.003131973324343562 | train_acc: 0.99862 | test_acc: 0.8929\n",
            "epoch: 221 |train_loss: 0.02212085947394371 | train_acc: 0.99854 | test_acc: 0.893\n",
            "epoch: 222 |train_loss: 0.005239963065832853 | train_acc: 0.9986 | test_acc: 0.8929\n",
            "epoch: 223 |train_loss: 0.003186503890901804 | train_acc: 0.99884 | test_acc: 0.8918\n",
            "epoch: 224 |train_loss: 0.04103114828467369 | train_acc: 0.99888 | test_acc: 0.8921\n",
            "epoch: 225 |train_loss: 0.002987371291965246 | train_acc: 0.9988 | test_acc: 0.8916\n",
            "epoch: 226 |train_loss: 0.01132366806268692 | train_acc: 0.99902 | test_acc: 0.8931\n",
            "epoch: 227 |train_loss: 0.00253809941932559 | train_acc: 0.99906 | test_acc: 0.8922\n",
            "epoch: 228 |train_loss: 0.001559853320941329 | train_acc: 0.99912 | test_acc: 0.8931\n",
            "epoch: 229 |train_loss: 0.0007333276444114745 | train_acc: 0.99892 | test_acc: 0.8935\n",
            "epoch: 230 |train_loss: 0.020641978830099106 | train_acc: 0.99906 | test_acc: 0.8945\n",
            "epoch: 231 |train_loss: 0.015283508226275444 | train_acc: 0.99922 | test_acc: 0.8925\n",
            "epoch: 232 |train_loss: 0.03136107325553894 | train_acc: 0.99906 | test_acc: 0.8924\n",
            "epoch: 233 |train_loss: 0.0010829624952748418 | train_acc: 0.99912 | test_acc: 0.8931\n",
            "epoch: 234 |train_loss: 0.004792171996086836 | train_acc: 0.99932 | test_acc: 0.8925\n",
            "epoch: 235 |train_loss: 0.0036493137013167143 | train_acc: 0.99912 | test_acc: 0.8926\n",
            "epoch: 236 |train_loss: 0.020142892375588417 | train_acc: 0.99914 | test_acc: 0.8914\n",
            "epoch: 237 |train_loss: 0.0010862848721444607 | train_acc: 0.99936 | test_acc: 0.8929\n",
            "epoch: 238 |train_loss: 0.009676385670900345 | train_acc: 0.99914 | test_acc: 0.8945\n",
            "epoch: 239 |train_loss: 0.0029971334151923656 | train_acc: 0.99926 | test_acc: 0.8942\n",
            "epoch: 240 |train_loss: 0.006037338636815548 | train_acc: 0.99958 | test_acc: 0.8951\n",
            "epoch: 241 |train_loss: 0.036069631576538086 | train_acc: 0.99914 | test_acc: 0.893\n",
            "epoch: 242 |train_loss: 0.0036499679554253817 | train_acc: 0.99916 | test_acc: 0.8936\n",
            "epoch: 243 |train_loss: 0.001062877126969397 | train_acc: 0.99914 | test_acc: 0.8948\n",
            "epoch: 244 |train_loss: 0.01248646154999733 | train_acc: 0.99926 | test_acc: 0.8942\n",
            "epoch: 245 |train_loss: 0.0017100826371461153 | train_acc: 0.99918 | test_acc: 0.8921\n",
            "epoch: 246 |train_loss: 0.007362539879977703 | train_acc: 0.99916 | test_acc: 0.8932\n",
            "epoch: 247 |train_loss: 0.012021131813526154 | train_acc: 0.99924 | test_acc: 0.8933\n",
            "epoch: 248 |train_loss: 0.004637222737073898 | train_acc: 0.99944 | test_acc: 0.8939\n",
            "epoch: 249 |train_loss: 0.002196224872022867 | train_acc: 0.99936 | test_acc: 0.8923\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  )\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (9): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (10): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (11): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (12): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (13): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): GlobalAvgPool2d()\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lolGNGVXasGd",
        "outputId": "8957f899-7c2b-4aa8-d4aa-904c5540c38e"
      },
      "source": [
        "model_basic2 = ResNet(BasicBlock, [3, 4, 14, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,num_classes=10,\n",
        "                   deep_stem=True, stem_width=32, avg_down=True,\n",
        "                   avd=True, avd_first=False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_basic2.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "results_file = f'results/resnest50_basic.csv'\n",
        "#model_file = f'pretrained/resnet{6*n+2}.pt'\n",
        "train(model_basic2, epochs, train_loader, test_loader, criterion, \n",
        "      optimizer, results_file, scheduler=scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "epoch: 0 |train_loss: 2.335160732269287 | train_acc: 0.10184 | test_acc: 0.1007\n",
            "epoch: 1 |train_loss: 2.1406619548797607 | train_acc: 0.16926 | test_acc: 0.1549\n",
            "epoch: 2 |train_loss: 1.9714727401733398 | train_acc: 0.15304 | test_acc: 0.1506\n",
            "epoch: 3 |train_loss: 1.7291923761367798 | train_acc: 0.27522 | test_acc: 0.2914\n",
            "epoch: 4 |train_loss: 1.9194997549057007 | train_acc: 0.32032 | test_acc: 0.3487\n",
            "epoch: 5 |train_loss: 1.738511323928833 | train_acc: 0.338 | test_acc: 0.3517\n",
            "epoch: 6 |train_loss: 1.7082706689834595 | train_acc: 0.36368 | test_acc: 0.3768\n",
            "epoch: 7 |train_loss: 1.756751298904419 | train_acc: 0.3253 | test_acc: 0.3451\n",
            "epoch: 8 |train_loss: 1.4442851543426514 | train_acc: 0.37408 | test_acc: 0.3946\n",
            "epoch: 9 |train_loss: 1.4072989225387573 | train_acc: 0.37126 | test_acc: 0.399\n",
            "epoch: 10 |train_loss: 1.4833457469940186 | train_acc: 0.44812 | test_acc: 0.4701\n",
            "epoch: 11 |train_loss: 1.5786921977996826 | train_acc: 0.42758 | test_acc: 0.4335\n",
            "epoch: 12 |train_loss: 1.4445148706436157 | train_acc: 0.38414 | test_acc: 0.4102\n",
            "epoch: 13 |train_loss: 1.3683375120162964 | train_acc: 0.51562 | test_acc: 0.5191\n",
            "epoch: 14 |train_loss: 1.1730581521987915 | train_acc: 0.51794 | test_acc: 0.5181\n",
            "epoch: 15 |train_loss: 1.2383745908737183 | train_acc: 0.55054 | test_acc: 0.5613\n",
            "epoch: 16 |train_loss: 1.172550082206726 | train_acc: 0.49608 | test_acc: 0.4922\n",
            "epoch: 17 |train_loss: 1.089963674545288 | train_acc: 0.5505 | test_acc: 0.5628\n",
            "epoch: 18 |train_loss: 1.3251397609710693 | train_acc: 0.5209 | test_acc: 0.523\n",
            "epoch: 19 |train_loss: 1.000773549079895 | train_acc: 0.55636 | test_acc: 0.5658\n",
            "epoch: 20 |train_loss: 1.2281665802001953 | train_acc: 0.57582 | test_acc: 0.5775\n",
            "epoch: 21 |train_loss: 0.9922327995300293 | train_acc: 0.56558 | test_acc: 0.5914\n",
            "epoch: 22 |train_loss: 0.7599384188652039 | train_acc: 0.65028 | test_acc: 0.6584\n",
            "epoch: 23 |train_loss: 0.864115834236145 | train_acc: 0.66116 | test_acc: 0.6681\n",
            "epoch: 24 |train_loss: 0.7450670003890991 | train_acc: 0.65958 | test_acc: 0.6804\n",
            "epoch: 25 |train_loss: 0.8977047204971313 | train_acc: 0.7011 | test_acc: 0.7074\n",
            "epoch: 26 |train_loss: 0.8118893504142761 | train_acc: 0.67118 | test_acc: 0.6738\n",
            "epoch: 27 |train_loss: 0.7033176422119141 | train_acc: 0.73208 | test_acc: 0.7366\n",
            "epoch: 28 |train_loss: 0.864672064781189 | train_acc: 0.68186 | test_acc: 0.6704\n",
            "epoch: 29 |train_loss: 0.671313464641571 | train_acc: 0.71986 | test_acc: 0.709\n",
            "epoch: 30 |train_loss: 0.8707027435302734 | train_acc: 0.73322 | test_acc: 0.7366\n",
            "epoch: 31 |train_loss: 0.7986758947372437 | train_acc: 0.73338 | test_acc: 0.7245\n",
            "epoch: 32 |train_loss: 0.5915048122406006 | train_acc: 0.72258 | test_acc: 0.7123\n",
            "epoch: 33 |train_loss: 0.5468004941940308 | train_acc: 0.7657 | test_acc: 0.741\n",
            "epoch: 34 |train_loss: 0.6874968409538269 | train_acc: 0.77006 | test_acc: 0.7534\n",
            "epoch: 35 |train_loss: 0.6203414797782898 | train_acc: 0.75722 | test_acc: 0.7404\n",
            "epoch: 36 |train_loss: 0.5725079774856567 | train_acc: 0.7501 | test_acc: 0.7351\n",
            "epoch: 37 |train_loss: 0.5555598735809326 | train_acc: 0.7775 | test_acc: 0.7558\n",
            "epoch: 38 |train_loss: 0.5835005044937134 | train_acc: 0.76868 | test_acc: 0.7503\n",
            "epoch: 39 |train_loss: 0.5302147269248962 | train_acc: 0.78186 | test_acc: 0.7601\n",
            "epoch: 40 |train_loss: 0.6136575937271118 | train_acc: 0.77426 | test_acc: 0.747\n",
            "epoch: 41 |train_loss: 0.8535836338996887 | train_acc: 0.63568 | test_acc: 0.6129\n",
            "epoch: 42 |train_loss: 0.6656818985939026 | train_acc: 0.81544 | test_acc: 0.7958\n",
            "epoch: 43 |train_loss: 0.3789863884449005 | train_acc: 0.80602 | test_acc: 0.7833\n",
            "epoch: 44 |train_loss: 0.7284749150276184 | train_acc: 0.7704 | test_acc: 0.7628\n",
            "epoch: 45 |train_loss: 0.6016485095024109 | train_acc: 0.75434 | test_acc: 0.7349\n",
            "epoch: 46 |train_loss: 0.712028443813324 | train_acc: 0.79666 | test_acc: 0.7787\n",
            "epoch: 47 |train_loss: 0.5893781781196594 | train_acc: 0.74042 | test_acc: 0.7232\n",
            "epoch: 48 |train_loss: 0.45177537202835083 | train_acc: 0.79298 | test_acc: 0.7704\n",
            "epoch: 49 |train_loss: 0.48038679361343384 | train_acc: 0.80072 | test_acc: 0.7764\n",
            "epoch: 50 |train_loss: 0.6494275331497192 | train_acc: 0.78746 | test_acc: 0.7643\n",
            "epoch: 51 |train_loss: 0.7314475774765015 | train_acc: 0.8228 | test_acc: 0.7882\n",
            "epoch: 52 |train_loss: 0.4951659142971039 | train_acc: 0.8314 | test_acc: 0.8034\n",
            "epoch: 53 |train_loss: 0.5331586599349976 | train_acc: 0.83898 | test_acc: 0.7996\n",
            "epoch: 54 |train_loss: 0.6019864082336426 | train_acc: 0.83696 | test_acc: 0.7858\n",
            "epoch: 55 |train_loss: 0.33575117588043213 | train_acc: 0.81722 | test_acc: 0.7748\n",
            "epoch: 56 |train_loss: 0.4935966432094574 | train_acc: 0.81826 | test_acc: 0.7796\n",
            "epoch: 57 |train_loss: 0.5437660813331604 | train_acc: 0.83412 | test_acc: 0.7942\n",
            "epoch: 58 |train_loss: 0.7001244425773621 | train_acc: 0.8113 | test_acc: 0.7729\n",
            "epoch: 59 |train_loss: 0.349547803401947 | train_acc: 0.82828 | test_acc: 0.784\n",
            "epoch: 60 |train_loss: 0.28443944454193115 | train_acc: 0.85656 | test_acc: 0.8146\n",
            "epoch: 61 |train_loss: 0.44827818870544434 | train_acc: 0.85614 | test_acc: 0.808\n",
            "epoch: 62 |train_loss: 0.2884364426136017 | train_acc: 0.85316 | test_acc: 0.81\n",
            "epoch: 63 |train_loss: 0.5442260503768921 | train_acc: 0.84042 | test_acc: 0.7997\n",
            "epoch: 64 |train_loss: 0.33809584379196167 | train_acc: 0.85612 | test_acc: 0.8134\n",
            "epoch: 65 |train_loss: 0.5485535264015198 | train_acc: 0.8433 | test_acc: 0.7935\n",
            "epoch: 66 |train_loss: 0.1719001829624176 | train_acc: 0.84686 | test_acc: 0.8062\n",
            "epoch: 67 |train_loss: 0.35090649127960205 | train_acc: 0.84488 | test_acc: 0.8074\n",
            "epoch: 68 |train_loss: 0.3068246841430664 | train_acc: 0.85956 | test_acc: 0.8145\n",
            "epoch: 69 |train_loss: 0.29064518213272095 | train_acc: 0.87196 | test_acc: 0.8232\n",
            "epoch: 70 |train_loss: 0.20632633566856384 | train_acc: 0.8639 | test_acc: 0.8068\n",
            "epoch: 71 |train_loss: 0.45303645730018616 | train_acc: 0.8483 | test_acc: 0.802\n",
            "epoch: 72 |train_loss: 0.4244210124015808 | train_acc: 0.86558 | test_acc: 0.8146\n",
            "epoch: 73 |train_loss: 0.42164188623428345 | train_acc: 0.86456 | test_acc: 0.8164\n",
            "epoch: 74 |train_loss: 0.3370446562767029 | train_acc: 0.84674 | test_acc: 0.7995\n",
            "epoch: 75 |train_loss: 0.21727974712848663 | train_acc: 0.86498 | test_acc: 0.8109\n",
            "epoch: 76 |train_loss: 0.3449031412601471 | train_acc: 0.8607 | test_acc: 0.8103\n",
            "epoch: 77 |train_loss: 0.43560418486595154 | train_acc: 0.86552 | test_acc: 0.8147\n",
            "epoch: 78 |train_loss: 0.39645373821258545 | train_acc: 0.843 | test_acc: 0.7965\n",
            "epoch: 79 |train_loss: 0.3405829668045044 | train_acc: 0.85942 | test_acc: 0.8058\n",
            "epoch: 80 |train_loss: 0.18310560286045074 | train_acc: 0.94508 | test_acc: 0.8756\n",
            "epoch: 81 |train_loss: 0.15684130787849426 | train_acc: 0.9534 | test_acc: 0.8776\n",
            "epoch: 82 |train_loss: 0.25168153643608093 | train_acc: 0.9566 | test_acc: 0.8772\n",
            "epoch: 83 |train_loss: 0.1888430416584015 | train_acc: 0.9602 | test_acc: 0.8782\n",
            "epoch: 84 |train_loss: 0.09470897167921066 | train_acc: 0.9635 | test_acc: 0.879\n",
            "epoch: 85 |train_loss: 0.18742170929908752 | train_acc: 0.96488 | test_acc: 0.8777\n",
            "epoch: 86 |train_loss: 0.11735150963068008 | train_acc: 0.96646 | test_acc: 0.8749\n",
            "epoch: 87 |train_loss: 0.13389717042446136 | train_acc: 0.9676 | test_acc: 0.8739\n",
            "epoch: 88 |train_loss: 0.10610812902450562 | train_acc: 0.97032 | test_acc: 0.8777\n",
            "epoch: 89 |train_loss: 0.03512071073055267 | train_acc: 0.97182 | test_acc: 0.8784\n",
            "epoch: 90 |train_loss: 0.11873028427362442 | train_acc: 0.97372 | test_acc: 0.8765\n",
            "epoch: 91 |train_loss: 0.07081449031829834 | train_acc: 0.97542 | test_acc: 0.8773\n",
            "epoch: 92 |train_loss: 0.12102138996124268 | train_acc: 0.97612 | test_acc: 0.8794\n",
            "epoch: 93 |train_loss: 0.20160503685474396 | train_acc: 0.97748 | test_acc: 0.8782\n",
            "epoch: 94 |train_loss: 0.1363322138786316 | train_acc: 0.9786 | test_acc: 0.878\n",
            "epoch: 95 |train_loss: 0.046908337622880936 | train_acc: 0.97804 | test_acc: 0.8777\n",
            "epoch: 96 |train_loss: 0.03829355165362358 | train_acc: 0.9792 | test_acc: 0.876\n",
            "epoch: 97 |train_loss: 0.042457133531570435 | train_acc: 0.9798 | test_acc: 0.8761\n",
            "epoch: 98 |train_loss: 0.017550835385918617 | train_acc: 0.98088 | test_acc: 0.8751\n",
            "epoch: 99 |train_loss: 0.06826204806566238 | train_acc: 0.98226 | test_acc: 0.8768\n",
            "epoch: 100 |train_loss: 0.0266463253647089 | train_acc: 0.9832 | test_acc: 0.878\n",
            "epoch: 101 |train_loss: 0.010450257919728756 | train_acc: 0.98162 | test_acc: 0.8757\n",
            "epoch: 102 |train_loss: 0.06337960064411163 | train_acc: 0.985 | test_acc: 0.8759\n",
            "epoch: 103 |train_loss: 0.1584877073764801 | train_acc: 0.98478 | test_acc: 0.8769\n",
            "epoch: 104 |train_loss: 0.06364233791828156 | train_acc: 0.98558 | test_acc: 0.8782\n",
            "epoch: 105 |train_loss: 0.04547443985939026 | train_acc: 0.98536 | test_acc: 0.8748\n",
            "epoch: 106 |train_loss: 0.14356538653373718 | train_acc: 0.98582 | test_acc: 0.8731\n",
            "epoch: 107 |train_loss: 0.025329465046525 | train_acc: 0.98576 | test_acc: 0.8744\n",
            "epoch: 108 |train_loss: 0.09838776290416718 | train_acc: 0.9871 | test_acc: 0.8746\n",
            "epoch: 109 |train_loss: 0.02317095175385475 | train_acc: 0.98602 | test_acc: 0.8749\n",
            "epoch: 110 |train_loss: 0.055132489651441574 | train_acc: 0.98746 | test_acc: 0.8742\n",
            "epoch: 111 |train_loss: 0.03339453786611557 | train_acc: 0.9867 | test_acc: 0.8781\n",
            "epoch: 112 |train_loss: 0.08988893032073975 | train_acc: 0.98838 | test_acc: 0.8756\n",
            "epoch: 113 |train_loss: 0.026253055781126022 | train_acc: 0.98818 | test_acc: 0.8761\n",
            "epoch: 114 |train_loss: 0.06517280638217926 | train_acc: 0.98988 | test_acc: 0.8753\n",
            "epoch: 115 |train_loss: 0.09683234989643097 | train_acc: 0.98826 | test_acc: 0.8751\n",
            "epoch: 116 |train_loss: 0.019658446311950684 | train_acc: 0.98802 | test_acc: 0.8767\n",
            "epoch: 117 |train_loss: 0.03140292689204216 | train_acc: 0.9892 | test_acc: 0.8719\n",
            "epoch: 118 |train_loss: 0.0679549127817154 | train_acc: 0.98954 | test_acc: 0.8747\n",
            "epoch: 119 |train_loss: 0.1371106505393982 | train_acc: 0.9881 | test_acc: 0.8745\n",
            "epoch: 120 |train_loss: 0.026405226439237595 | train_acc: 0.99024 | test_acc: 0.8758\n",
            "epoch: 121 |train_loss: 0.03797116130590439 | train_acc: 0.98844 | test_acc: 0.8724\n",
            "epoch: 122 |train_loss: 0.10021892935037613 | train_acc: 0.98894 | test_acc: 0.8748\n",
            "epoch: 123 |train_loss: 0.025615166872739792 | train_acc: 0.99096 | test_acc: 0.8738\n",
            "epoch: 124 |train_loss: 0.011675701476633549 | train_acc: 0.9905 | test_acc: 0.8717\n",
            "epoch: 125 |train_loss: 0.038002848625183105 | train_acc: 0.9912 | test_acc: 0.877\n",
            "epoch: 126 |train_loss: 0.048822615295648575 | train_acc: 0.99036 | test_acc: 0.8743\n",
            "epoch: 127 |train_loss: 0.041375160217285156 | train_acc: 0.99086 | test_acc: 0.8767\n",
            "epoch: 128 |train_loss: 0.02275165356695652 | train_acc: 0.99046 | test_acc: 0.8763\n",
            "epoch: 129 |train_loss: 0.08141627907752991 | train_acc: 0.98946 | test_acc: 0.8731\n",
            "epoch: 130 |train_loss: 0.13522380590438843 | train_acc: 0.9904 | test_acc: 0.8735\n",
            "epoch: 131 |train_loss: 0.07383692264556885 | train_acc: 0.99006 | test_acc: 0.8727\n",
            "epoch: 132 |train_loss: 0.06138472631573677 | train_acc: 0.99146 | test_acc: 0.8752\n",
            "epoch: 133 |train_loss: 0.034054726362228394 | train_acc: 0.99014 | test_acc: 0.8757\n",
            "epoch: 134 |train_loss: 0.009436007589101791 | train_acc: 0.9908 | test_acc: 0.875\n",
            "epoch: 135 |train_loss: 0.024143125861883163 | train_acc: 0.9917 | test_acc: 0.872\n",
            "epoch: 136 |train_loss: 0.008235687389969826 | train_acc: 0.99074 | test_acc: 0.8721\n",
            "epoch: 137 |train_loss: 0.035977479070425034 | train_acc: 0.99122 | test_acc: 0.8748\n",
            "epoch: 138 |train_loss: 0.007211835123598576 | train_acc: 0.99218 | test_acc: 0.8759\n",
            "epoch: 139 |train_loss: 0.05146215111017227 | train_acc: 0.99114 | test_acc: 0.8751\n",
            "epoch: 140 |train_loss: 0.014193950220942497 | train_acc: 0.99122 | test_acc: 0.8738\n",
            "epoch: 141 |train_loss: 0.03769219294190407 | train_acc: 0.99118 | test_acc: 0.874\n",
            "epoch: 142 |train_loss: 0.0038991481997072697 | train_acc: 0.98982 | test_acc: 0.8746\n",
            "epoch: 143 |train_loss: 0.014854597859084606 | train_acc: 0.99216 | test_acc: 0.8767\n",
            "epoch: 144 |train_loss: 0.025564294308423996 | train_acc: 0.99178 | test_acc: 0.8767\n",
            "epoch: 145 |train_loss: 0.0229653250426054 | train_acc: 0.99108 | test_acc: 0.8758\n",
            "epoch: 146 |train_loss: 0.019649237394332886 | train_acc: 0.99182 | test_acc: 0.8769\n",
            "epoch: 147 |train_loss: 0.04694778472185135 | train_acc: 0.99116 | test_acc: 0.8751\n",
            "epoch: 148 |train_loss: 0.05555909126996994 | train_acc: 0.9905 | test_acc: 0.8719\n",
            "epoch: 149 |train_loss: 0.08918727934360504 | train_acc: 0.99204 | test_acc: 0.8735\n",
            "epoch: 150 |train_loss: 0.05466919019818306 | train_acc: 0.99234 | test_acc: 0.8747\n",
            "epoch: 151 |train_loss: 0.06247594952583313 | train_acc: 0.99152 | test_acc: 0.8732\n",
            "epoch: 152 |train_loss: 0.041844528168439865 | train_acc: 0.99202 | test_acc: 0.8748\n",
            "epoch: 153 |train_loss: 0.07930399477481842 | train_acc: 0.99144 | test_acc: 0.8753\n",
            "epoch: 154 |train_loss: 0.09618248045444489 | train_acc: 0.99224 | test_acc: 0.8716\n",
            "epoch: 155 |train_loss: 0.0026272526010870934 | train_acc: 0.9928 | test_acc: 0.8715\n",
            "epoch: 156 |train_loss: 0.07196170836687088 | train_acc: 0.99228 | test_acc: 0.872\n",
            "epoch: 157 |train_loss: 0.038409311324357986 | train_acc: 0.9939 | test_acc: 0.8776\n",
            "epoch: 158 |train_loss: 0.07858237624168396 | train_acc: 0.9926 | test_acc: 0.8713\n",
            "epoch: 159 |train_loss: 0.0036150093656033278 | train_acc: 0.99152 | test_acc: 0.874\n",
            "epoch: 160 |train_loss: 0.015478159300982952 | train_acc: 0.9917 | test_acc: 0.8773\n",
            "epoch: 161 |train_loss: 0.01871119625866413 | train_acc: 0.99118 | test_acc: 0.8732\n",
            "epoch: 162 |train_loss: 0.034376490861177444 | train_acc: 0.9924 | test_acc: 0.8742\n",
            "epoch: 163 |train_loss: 0.0019155420595780015 | train_acc: 0.99196 | test_acc: 0.8751\n",
            "epoch: 164 |train_loss: 0.01131560280919075 | train_acc: 0.99336 | test_acc: 0.876\n",
            "epoch: 165 |train_loss: 0.008964579552412033 | train_acc: 0.99294 | test_acc: 0.8774\n",
            "epoch: 166 |train_loss: 0.08350878208875656 | train_acc: 0.9928 | test_acc: 0.8755\n",
            "epoch: 167 |train_loss: 0.052051741629838943 | train_acc: 0.99308 | test_acc: 0.8765\n",
            "epoch: 168 |train_loss: 0.07274453341960907 | train_acc: 0.99304 | test_acc: 0.8768\n",
            "epoch: 169 |train_loss: 0.024641085416078568 | train_acc: 0.99346 | test_acc: 0.8757\n",
            "epoch: 170 |train_loss: 0.01857336238026619 | train_acc: 0.99238 | test_acc: 0.8757\n",
            "epoch: 171 |train_loss: 0.01885644719004631 | train_acc: 0.9924 | test_acc: 0.877\n",
            "epoch: 172 |train_loss: 0.014124678447842598 | train_acc: 0.99336 | test_acc: 0.877\n",
            "epoch: 173 |train_loss: 0.002692395355552435 | train_acc: 0.99042 | test_acc: 0.8751\n",
            "epoch: 174 |train_loss: 0.03596403822302818 | train_acc: 0.99126 | test_acc: 0.8726\n",
            "epoch: 175 |train_loss: 0.06371109932661057 | train_acc: 0.99248 | test_acc: 0.8757\n",
            "epoch: 176 |train_loss: 0.028748756274580956 | train_acc: 0.991 | test_acc: 0.8738\n",
            "epoch: 177 |train_loss: 0.021819021552801132 | train_acc: 0.9923 | test_acc: 0.8767\n",
            "epoch: 178 |train_loss: 0.02712704800069332 | train_acc: 0.9928 | test_acc: 0.8774\n",
            "epoch: 179 |train_loss: 0.05405436083674431 | train_acc: 0.99288 | test_acc: 0.876\n",
            "epoch: 180 |train_loss: 0.03393413871526718 | train_acc: 0.99264 | test_acc: 0.8721\n",
            "epoch: 181 |train_loss: 0.1217491403222084 | train_acc: 0.99284 | test_acc: 0.8759\n",
            "epoch: 182 |train_loss: 0.02091130055487156 | train_acc: 0.99164 | test_acc: 0.8747\n",
            "epoch: 183 |train_loss: 0.0253501795232296 | train_acc: 0.99208 | test_acc: 0.8757\n",
            "epoch: 184 |train_loss: 0.008661464788019657 | train_acc: 0.99072 | test_acc: 0.8707\n",
            "epoch: 185 |train_loss: 0.038859881460666656 | train_acc: 0.99088 | test_acc: 0.8747\n",
            "epoch: 186 |train_loss: 0.08006314188241959 | train_acc: 0.99124 | test_acc: 0.8717\n",
            "epoch: 187 |train_loss: 0.028537040576338768 | train_acc: 0.99254 | test_acc: 0.8732\n",
            "epoch: 188 |train_loss: 0.005170827265828848 | train_acc: 0.99374 | test_acc: 0.8745\n",
            "epoch: 189 |train_loss: 0.004827377386391163 | train_acc: 0.99222 | test_acc: 0.8753\n",
            "epoch: 190 |train_loss: 0.0636952668428421 | train_acc: 0.99388 | test_acc: 0.8735\n",
            "epoch: 191 |train_loss: 0.028228094801306725 | train_acc: 0.99182 | test_acc: 0.8724\n",
            "epoch: 192 |train_loss: 0.015336486510932446 | train_acc: 0.99224 | test_acc: 0.8753\n",
            "epoch: 193 |train_loss: 0.07032397389411926 | train_acc: 0.99088 | test_acc: 0.8723\n",
            "epoch: 194 |train_loss: 0.046927787363529205 | train_acc: 0.99048 | test_acc: 0.8724\n",
            "epoch: 195 |train_loss: 0.041361190378665924 | train_acc: 0.99164 | test_acc: 0.8707\n",
            "epoch: 196 |train_loss: 0.06703855842351913 | train_acc: 0.99094 | test_acc: 0.8756\n",
            "epoch: 197 |train_loss: 0.15616413950920105 | train_acc: 0.9906 | test_acc: 0.872\n",
            "epoch: 198 |train_loss: 0.0243096761405468 | train_acc: 0.99098 | test_acc: 0.8711\n",
            "epoch: 199 |train_loss: 0.015512417070567608 | train_acc: 0.99184 | test_acc: 0.874\n",
            "epoch: 200 |train_loss: 0.05963285639882088 | train_acc: 0.99226 | test_acc: 0.8724\n",
            "epoch: 201 |train_loss: 0.01613164320588112 | train_acc: 0.9935 | test_acc: 0.8777\n",
            "epoch: 202 |train_loss: 0.028376823291182518 | train_acc: 0.98924 | test_acc: 0.8704\n",
            "epoch: 203 |train_loss: 0.06521730124950409 | train_acc: 0.99072 | test_acc: 0.8725\n",
            "epoch: 204 |train_loss: 0.017340783029794693 | train_acc: 0.99206 | test_acc: 0.8732\n",
            "epoch: 205 |train_loss: 0.11394189298152924 | train_acc: 0.9915 | test_acc: 0.8712\n",
            "epoch: 206 |train_loss: 0.051698409020900726 | train_acc: 0.99108 | test_acc: 0.8758\n",
            "epoch: 207 |train_loss: 0.05306132882833481 | train_acc: 0.98962 | test_acc: 0.8714\n",
            "epoch: 208 |train_loss: 0.02456023544073105 | train_acc: 0.99112 | test_acc: 0.8734\n",
            "epoch: 209 |train_loss: 0.011893050745129585 | train_acc: 0.99166 | test_acc: 0.8732\n",
            "epoch: 210 |train_loss: 0.012348507530987263 | train_acc: 0.99686 | test_acc: 0.8824\n",
            "epoch: 211 |train_loss: 0.003595661371946335 | train_acc: 0.99764 | test_acc: 0.8833\n",
            "epoch: 212 |train_loss: 0.003863014979287982 | train_acc: 0.9977 | test_acc: 0.8838\n",
            "epoch: 213 |train_loss: 0.008280914276838303 | train_acc: 0.99792 | test_acc: 0.8855\n",
            "epoch: 214 |train_loss: 0.03504731506109238 | train_acc: 0.9981 | test_acc: 0.8858\n",
            "epoch: 215 |train_loss: 0.054935771971940994 | train_acc: 0.99824 | test_acc: 0.8854\n",
            "epoch: 216 |train_loss: 0.02588891051709652 | train_acc: 0.99878 | test_acc: 0.8857\n",
            "epoch: 217 |train_loss: 0.003357183188199997 | train_acc: 0.9986 | test_acc: 0.8854\n",
            "epoch: 218 |train_loss: 0.023548128083348274 | train_acc: 0.99876 | test_acc: 0.8856\n",
            "epoch: 219 |train_loss: 0.017238570377230644 | train_acc: 0.99888 | test_acc: 0.8849\n",
            "epoch: 220 |train_loss: 0.0054525346495211124 | train_acc: 0.99894 | test_acc: 0.885\n",
            "epoch: 221 |train_loss: 0.028033960610628128 | train_acc: 0.99852 | test_acc: 0.8845\n",
            "epoch: 222 |train_loss: 0.004841731395572424 | train_acc: 0.99894 | test_acc: 0.8861\n",
            "epoch: 223 |train_loss: 0.0006253835163079202 | train_acc: 0.99906 | test_acc: 0.8854\n",
            "epoch: 224 |train_loss: 0.002106982748955488 | train_acc: 0.99898 | test_acc: 0.8854\n",
            "epoch: 225 |train_loss: 0.02284419909119606 | train_acc: 0.99908 | test_acc: 0.8853\n",
            "epoch: 226 |train_loss: 0.0026378442998975515 | train_acc: 0.99912 | test_acc: 0.8845\n",
            "epoch: 227 |train_loss: 0.002694967668503523 | train_acc: 0.99914 | test_acc: 0.8859\n",
            "epoch: 228 |train_loss: 0.004691302310675383 | train_acc: 0.9992 | test_acc: 0.8866\n",
            "epoch: 229 |train_loss: 0.001487238216213882 | train_acc: 0.99922 | test_acc: 0.8853\n",
            "epoch: 230 |train_loss: 0.005940325558185577 | train_acc: 0.99916 | test_acc: 0.8868\n",
            "epoch: 231 |train_loss: 0.019572552293539047 | train_acc: 0.9995 | test_acc: 0.8849\n",
            "epoch: 232 |train_loss: 0.004371549002826214 | train_acc: 0.99914 | test_acc: 0.8861\n",
            "epoch: 233 |train_loss: 0.014553139917552471 | train_acc: 0.99936 | test_acc: 0.8873\n",
            "epoch: 234 |train_loss: 0.002159649273380637 | train_acc: 0.99934 | test_acc: 0.8868\n",
            "epoch: 235 |train_loss: 0.00846909824758768 | train_acc: 0.99936 | test_acc: 0.8855\n",
            "epoch: 236 |train_loss: 0.0012983015039935708 | train_acc: 0.99924 | test_acc: 0.8861\n",
            "epoch: 237 |train_loss: 0.0020097142551094294 | train_acc: 0.99944 | test_acc: 0.8877\n",
            "epoch: 238 |train_loss: 0.014736329205334187 | train_acc: 0.9994 | test_acc: 0.8855\n",
            "epoch: 239 |train_loss: 0.05485522747039795 | train_acc: 0.9993 | test_acc: 0.8864\n",
            "epoch: 240 |train_loss: 0.011488338932394981 | train_acc: 0.9996 | test_acc: 0.8853\n",
            "epoch: 241 |train_loss: 0.03191216662526131 | train_acc: 0.9995 | test_acc: 0.8862\n",
            "epoch: 242 |train_loss: 0.007441245950758457 | train_acc: 0.99952 | test_acc: 0.8869\n",
            "epoch: 243 |train_loss: 0.007634551730006933 | train_acc: 0.99946 | test_acc: 0.8868\n",
            "epoch: 244 |train_loss: 0.0022213689517229795 | train_acc: 0.99948 | test_acc: 0.8863\n",
            "epoch: 245 |train_loss: 0.006324313580989838 | train_acc: 0.99942 | test_acc: 0.8863\n",
            "epoch: 246 |train_loss: 0.0015652267029508948 | train_acc: 0.99958 | test_acc: 0.8868\n",
            "epoch: 247 |train_loss: 0.014906617812812328 | train_acc: 0.99938 | test_acc: 0.8871\n",
            "epoch: 248 |train_loss: 0.0025415485724806786 | train_acc: 0.99966 | test_acc: 0.8864\n",
            "epoch: 249 |train_loss: 0.00220569153316319 | train_acc: 0.99956 | test_acc: 0.8862\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  )\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (6): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (7): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (8): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (9): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (10): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (11): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (12): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (13): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): SplAtConv2d(\n",
              "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
              "        (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (rsoftmax): rSoftMax()\n",
              "      )\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): GlobalAvgPool2d()\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIcPAr1X9Z2d"
      },
      "source": [
        "### Bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoSni-O6rMek"
      },
      "source": [
        "model = ResNet(Bottleneck, [3, 4, 6, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,num_classes=100,\n",
        "                   deep_stem=True, stem_width=32, avg_down=True,\n",
        "                   avd=True, avd_first=False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "results_file = 'resnest50_bottleneck_p100.csv'\n",
        "model_file = 'resnest50_bottleneck_p100.pt'\n",
        "train(model, epochs, train_loader, test_loader, criterion, \n",
        "      optimizer, results_file, scheduler=scheduler, MODEL_PATH=model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKUj1BS7XncH"
      },
      "source": [
        "model2 = ResNet(Bottleneck, [3, 4, 6, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,num_classes=100,\n",
        "                   deep_stem=True, stem_width=32, avg_down=True,\n",
        "                   avd=True, avd_first=False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model2.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "results_file = 'resnest50_bottleneck_V100.csv'\n",
        "model_file = 'resnest50_bottleneck_V100.pt'\n",
        "train(model2, epochs, train_loader, test_loader, criterion, \n",
        "      optimizer, results_file, scheduler=scheduler, MODEL_PATH=model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0wMfiaVMN2c"
      },
      "source": [
        "# plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w38vxNpXnZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ad5b00-2dd6-42a0-f7f9-891271502293"
      },
      "source": [
        "cd drive/MyDrive/Colab\\ Notebooks/CS3033\\ Final\\ Project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1ys85tT914ph5XYcchx1AzNxqHH3eO0Qe/CS3033 Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hr9TBOHONbEm"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "OHyOi6QnOQBu",
        "outputId": "2de40cde-5fdf-4cfb-cdfd-a43bf1ec9441"
      },
      "source": [
        "basic = pd.read_csv(\"results_hg/resnest50_basic.csv\")\n",
        "basic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>test_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2.338618</td>\n",
              "      <td>0.10402</td>\n",
              "      <td>0.1066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2.087804</td>\n",
              "      <td>0.20702</td>\n",
              "      <td>0.2150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.778952</td>\n",
              "      <td>0.27354</td>\n",
              "      <td>0.2860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.779820</td>\n",
              "      <td>0.31920</td>\n",
              "      <td>0.3198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.717509</td>\n",
              "      <td>0.35580</td>\n",
              "      <td>0.3684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>245</td>\n",
              "      <td>0.001710</td>\n",
              "      <td>0.99918</td>\n",
              "      <td>0.8921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>246</td>\n",
              "      <td>0.007363</td>\n",
              "      <td>0.99916</td>\n",
              "      <td>0.8932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>247</td>\n",
              "      <td>0.012021</td>\n",
              "      <td>0.99924</td>\n",
              "      <td>0.8933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>248</td>\n",
              "      <td>0.004637</td>\n",
              "      <td>0.99944</td>\n",
              "      <td>0.8939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>249</td>\n",
              "      <td>0.002196</td>\n",
              "      <td>0.99936</td>\n",
              "      <td>0.8923</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     epoch  train_loss  train_acc  test_acc\n",
              "0        0    2.338618    0.10402    0.1066\n",
              "1        1    2.087804    0.20702    0.2150\n",
              "2        2    1.778952    0.27354    0.2860\n",
              "3        3    1.779820    0.31920    0.3198\n",
              "4        4    1.717509    0.35580    0.3684\n",
              "..     ...         ...        ...       ...\n",
              "245    245    0.001710    0.99918    0.8921\n",
              "246    246    0.007363    0.99916    0.8932\n",
              "247    247    0.012021    0.99924    0.8933\n",
              "248    248    0.004637    0.99944    0.8939\n",
              "249    249    0.002196    0.99936    0.8923\n",
              "\n",
              "[250 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWzNfrrKXnT4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "4153272c-fc92-4b64-a0b1-4a7a3237c253"
      },
      "source": [
        "bottleneck = pd.read_csv(\"results_xf/resnest50_bottleneck_v100.csv\")\n",
        "bottleneck"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>train_acc</th>\n",
              "      <th>test_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2.3637</td>\n",
              "      <td>0.2014</td>\n",
              "      <td>0.2218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.8615</td>\n",
              "      <td>0.3115</td>\n",
              "      <td>0.3356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.7494</td>\n",
              "      <td>0.3551</td>\n",
              "      <td>0.3709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.3441</td>\n",
              "      <td>0.3887</td>\n",
              "      <td>0.4012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.5299</td>\n",
              "      <td>0.4240</td>\n",
              "      <td>0.4375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>245</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.9999</td>\n",
              "      <td>0.8754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>246</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>0.9999</td>\n",
              "      <td>0.8773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>247</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.9999</td>\n",
              "      <td>0.8756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>248</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>249</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.9999</td>\n",
              "      <td>0.8765</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     epoch  train_loss  train_acc  test_acc\n",
              "0        0      2.3637     0.2014    0.2218\n",
              "1        1      1.8615     0.3115    0.3356\n",
              "2        2      1.7494     0.3551    0.3709\n",
              "3        3      1.3441     0.3887    0.4012\n",
              "4        4      1.5299     0.4240    0.4375\n",
              "..     ...         ...        ...       ...\n",
              "245    245      0.0125     0.9999    0.8754\n",
              "246    246      0.0008     0.9999    0.8773\n",
              "247    247      0.0002     0.9999    0.8756\n",
              "248    248      0.0004     1.0000    0.8753\n",
              "249    249      0.0030     0.9999    0.8765\n",
              "\n",
              "[250 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L9W61WO3PnA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "fb2db435-0b20-40a8-d7d8-8ed70d94aba7"
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(bottleneck['epoch'], bottleneck['test_acc'], label=\"bottleneck\")\n",
        "plt.plot(basic['epoch'], basic['test_acc'], label=\"basic block\")\n",
        "plt.xlabel(\"# Epoch\")\n",
        "plt.ylabel(\"Test accuracy\")\n",
        "plt.title(\"Resnest50 on CIFAR10\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe0a279eb50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtiRXaGRPlOZ"
      },
      "source": [
        "## transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmC_gcIE3Pkl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "1bfc0398-9ef1-40ba-d2a8-6747f575898a"
      },
      "source": [
        "transfer_bottleneck = pd.read_csv(\"results_xf/transfer_bottleneck.csv\")\n",
        "transfer_bottleneck"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>test_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.1033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.1674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.2237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.2673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.3042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>195</td>\n",
              "      <td>0.6163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>196</td>\n",
              "      <td>0.6174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>197</td>\n",
              "      <td>0.6147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>198</td>\n",
              "      <td>0.6158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>199</td>\n",
              "      <td>0.6152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     epoch  test_acc\n",
              "0        0    0.1033\n",
              "1        1    0.1674\n",
              "2        2    0.2237\n",
              "3        3    0.2673\n",
              "4        4    0.3042\n",
              "..     ...       ...\n",
              "195    195    0.6163\n",
              "196    196    0.6174\n",
              "197    197    0.6147\n",
              "198    198    0.6158\n",
              "199    199    0.6152\n",
              "\n",
              "[200 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hazlgs9Z3Phx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "9d17d8a9-7eb1-4bbf-d084-7e7f80f26dae"
      },
      "source": [
        "transfer_basic = pd.read_csv(\"results_hg/transfer_basic.csv\")\n",
        "transfer_basic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>epoch</th>\n",
              "      <th>test_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.2090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.3008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.3681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.4048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.4436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>195</td>\n",
              "      <td>0.6303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>196</td>\n",
              "      <td>0.6288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>197</td>\n",
              "      <td>0.6312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>198</td>\n",
              "      <td>0.6317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>199</td>\n",
              "      <td>0.6323</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     epoch  test_acc\n",
              "0        0    0.2090\n",
              "1        1    0.3008\n",
              "2        2    0.3681\n",
              "3        3    0.4048\n",
              "4        4    0.4436\n",
              "..     ...       ...\n",
              "195    195    0.6303\n",
              "196    196    0.6288\n",
              "197    197    0.6312\n",
              "198    198    0.6317\n",
              "199    199    0.6323\n",
              "\n",
              "[200 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-1zKMC13PfF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "c77bfde9-208b-4f12-bd57-dddfb0b9575d"
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(transfer_basic['epoch'], transfer_basic['test_acc'], label=\"bottleneck\")\n",
        "\n",
        "plt.plot(transfer_bottleneck['epoch'], transfer_bottleneck['test_acc'], label=\"basic block\")\n",
        "\n",
        "plt.xlabel(\"# Epoch\")\n",
        "plt.ylabel(\"Test accuracy\")\n",
        "plt.title(\"Transfer Learning on CIFAR100\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe0a26c8d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyGGu-DZ3Pc3"
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(transfer_basic['epoch'], transfer_basic['test_acc'], label=\"basic block\")\n",
        "plt.plot(transfer_bottleneck['epoch'], transfer_bottleneck['test_acc'], label=\"bottleneck\")\n",
        "plt.xlabel(\"# Epoch\")\n",
        "plt.ylabel(\"Test accuracy\")\n",
        "plt.title(\"Transfer Learning on CIFAR100\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb9qUyM43PaK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD6dvRFE3PXN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc5-3Nxa3POM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}