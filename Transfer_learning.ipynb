{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer learning .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5EHhmpgGm85",
        "outputId": "b52ac69c-8e3d-42e1-da66-4fdf83d2a152"
      },
      "source": [
        "#cd /content/drive/MyDrive/CS3033 Final Project/results_hg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CS3033 Final Project/results_hg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VYBhfLnHsgG"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DtIi8NA8AJE",
        "outputId": "c3a181a1-9f38-4310-da8e-6be0ad6d3a1b"
      },
      "source": [
        "cd /content/drive/MyDrive/CS3033 Final Project/torch/models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CS3033 Final Project/torch/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KaSlkMO75dI"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from splat import SplAtConv2d, DropBlock2D\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZctSZ6R8Kxd"
      },
      "source": [
        "## Resnest basic block framework"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9YxTuvD8JeA"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"ResNet BasicBlock\n",
        "    \"\"\"\n",
        "    # pylint: disable=unused-argument\n",
        "    expansion = 1\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
        "                 radix=1, cardinality=1, bottleneck_width=64,\n",
        "                 avd=False, avd_first=False, dilation=1, is_first=False,\n",
        "                 rectified_conv=False, rectify_avg=False,\n",
        "                 norm_layer=None, dropblock_prob=0.0, last_gamma=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n",
        "        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = norm_layer(group_width)\n",
        "        self.dropblock_prob = dropblock_prob\n",
        "        self.radix = radix\n",
        "        self.avd = avd and (stride > 1 or is_first)\n",
        "        self.avd_first = avd_first\n",
        "\n",
        "        if self.avd:\n",
        "            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n",
        "            stride = 1\n",
        "\n",
        "        if dropblock_prob > 0.0:\n",
        "            self.dropblock1 = DropBlock2D(dropblock_prob, 3)\n",
        "            if radix == 1:\n",
        "                self.dropblock2 = DropBlock2D(dropblock_prob, 3)\n",
        "            self.dropblock3 = DropBlock2D(dropblock_prob, 3)\n",
        "\n",
        "        if radix >= 1:\n",
        "            self.conv2 = SplAtConv2d(\n",
        "                group_width, group_width, kernel_size=3,\n",
        "                stride=stride, padding=dilation,\n",
        "                dilation=dilation, groups=cardinality, bias=False,\n",
        "                radix=radix, rectify=rectified_conv,\n",
        "                rectify_avg=rectify_avg,\n",
        "                norm_layer=norm_layer,\n",
        "                dropblock_prob=dropblock_prob)\n",
        "        elif rectified_conv:\n",
        "            from rfconv import RFConv2d\n",
        "            self.conv2 = RFConv2d(\n",
        "                group_width, group_width, kernel_size=3, stride=stride,\n",
        "                padding=dilation, dilation=dilation,\n",
        "                groups=cardinality, bias=False,\n",
        "                average_mode=rectify_avg)\n",
        "            self.bn2 = norm_layer(group_width)\n",
        "        else:\n",
        "            self.conv2 = nn.Conv2d(\n",
        "                group_width, group_width, kernel_size=3, stride=stride,\n",
        "                padding=dilation, dilation=dilation,\n",
        "                groups=cardinality, bias=False)\n",
        "            self.bn2 = norm_layer(group_width)\n",
        "\n",
        "        # self.conv3 = nn.Conv2d(\n",
        "        #     group_width, planes * 4, kernel_size=1, bias=False)\n",
        "        # self.bn3 = norm_layer(planes*4)\n",
        "\n",
        "        if last_gamma:\n",
        "            from torch.nn.init import zeros_\n",
        "            zeros_(self.bn3.weight)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.dilation = dilation\n",
        "        self.stride = stride     \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            out = self.dropblock1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        if self.avd and self.avd_first:\n",
        "            out = self.avd_layer(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        if self.radix == 0:\n",
        "            out = self.bn2(out)\n",
        "            if self.dropblock_prob > 0.0:\n",
        "                out = self.dropblock2(out)\n",
        "            out = self.relu(out)\n",
        "\n",
        "        if self.avd and not self.avd_first:\n",
        "            out = self.avd_layer(out)\n",
        "\n",
        "        # out = self.conv3(out)\n",
        "        # out = self.bn3(out)\n",
        "        if self.dropblock_prob > 0.0:\n",
        "            out = self.dropblock3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1Ipp_Qo8X36"
      },
      "source": [
        "class GlobalAvgPool2d(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n",
        "        super(GlobalAvgPool2d, self).__init__()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return nn.functional.adaptive_avg_pool2d(inputs, 1).view(inputs.size(0), -1)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"ResNet Variants\n",
        "    Parameters\n",
        "    ----------\n",
        "    block : Block\n",
        "        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n",
        "    layers : list of int\n",
        "        Numbers of layers in each block\n",
        "    classes : int, default 1000\n",
        "        Number of classification classes.\n",
        "    dilated : bool, default False\n",
        "        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n",
        "        typically used in Semantic Segmentation.\n",
        "    norm_layer : object\n",
        "        Normalization layer used in backbone network (default: :class:`mxnet.gluon.nn.BatchNorm`;\n",
        "        for Synchronized Cross-GPU BachNormalization).\n",
        "    Reference:\n",
        "        - He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n",
        "        - Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\"\n",
        "    \"\"\"\n",
        "    # pylint: disable=unused-variable\n",
        "    def __init__(self, block, layers, radix=1, groups=1, bottleneck_width=64,\n",
        "                 num_classes=1000, dilated=False, dilation=1,\n",
        "                 deep_stem=False, stem_width=64, avg_down=False,\n",
        "                 rectified_conv=False, rectify_avg=False,\n",
        "                 avd=False, avd_first=False,\n",
        "                 final_drop=0.0, dropblock_prob=0,\n",
        "                 last_gamma=False, norm_layer=nn.BatchNorm2d):\n",
        "\n",
        "        self.cardinality = groups\n",
        "        self.bottleneck_width = bottleneck_width\n",
        "        # ResNet-D params\n",
        "        self.inplanes = stem_width*2 if deep_stem else 64\n",
        "        self.avg_down = avg_down\n",
        "        self.last_gamma = last_gamma\n",
        "        # ResNeSt params\n",
        "        self.radix = radix\n",
        "        self.avd = avd\n",
        "        self.avd_first = avd_first\n",
        "\n",
        "        super(ResNet, self).__init__()\n",
        "        self.rectified_conv = rectified_conv\n",
        "        self.rectify_avg = rectify_avg\n",
        "        if rectified_conv:\n",
        "            from rfconv import RFConv2d\n",
        "            conv_layer = RFConv2d\n",
        "        else:\n",
        "            conv_layer = nn.Conv2d\n",
        "        conv_kwargs = {'average_mode': rectify_avg} if rectified_conv else {}\n",
        "        if deep_stem:\n",
        "            self.conv1 = nn.Sequential(\n",
        "                conv_layer(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **conv_kwargs),\n",
        "                norm_layer(stem_width),\n",
        "                nn.ReLU(inplace=True),\n",
        "                conv_layer(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
        "                norm_layer(stem_width),\n",
        "                nn.ReLU(inplace=True),\n",
        "                conv_layer(stem_width, stem_width*2, kernel_size=3, stride=1, padding=1, bias=False, **conv_kwargs),\n",
        "            )\n",
        "        else:\n",
        "            self.conv1 = conv_layer(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                                   bias=False, **conv_kwargs)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], norm_layer=norm_layer, is_first=False)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, norm_layer=norm_layer)\n",
        "        if dilated or dilation == 4:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n",
        "                                           dilation=2, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
        "                                           dilation=4, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        elif dilation==2:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                           dilation=1, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n",
        "                                           dilation=2, norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        else:\n",
        "            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                           norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                           norm_layer=norm_layer,\n",
        "                                           dropblock_prob=dropblock_prob)\n",
        "        self.avgpool = GlobalAvgPool2d()\n",
        "        self.drop = nn.Dropout(final_drop) if final_drop > 0.0 else None\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, norm_layer):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_layer=None,\n",
        "                    dropblock_prob=0.0, is_first=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            down_layers = []\n",
        "            if self.avg_down:\n",
        "                if dilation == 1:\n",
        "                    down_layers.append(nn.AvgPool2d(kernel_size=stride, stride=stride,\n",
        "                                                    ceil_mode=True, count_include_pad=False))\n",
        "                else:\n",
        "                    down_layers.append(nn.AvgPool2d(kernel_size=1, stride=1,\n",
        "                                                    ceil_mode=True, count_include_pad=False))\n",
        "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                             kernel_size=1, stride=1, bias=False))\n",
        "            else:\n",
        "                down_layers.append(nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                                             kernel_size=1, stride=stride, bias=False))\n",
        "            down_layers.append(norm_layer(planes * block.expansion))\n",
        "            downsample = nn.Sequential(*down_layers)\n",
        "\n",
        "        layers = []\n",
        "        if dilation == 1 or dilation == 2:\n",
        "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=1, is_first=is_first, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "        elif dilation == 4:\n",
        "            layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=2, is_first=is_first, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "        else:\n",
        "            raise RuntimeError(\"=> unknown dilation size: {}\".format(dilation))\n",
        "\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes,\n",
        "                                radix=self.radix, cardinality=self.cardinality,\n",
        "                                bottleneck_width=self.bottleneck_width,\n",
        "                                avd=self.avd, avd_first=self.avd_first,\n",
        "                                dilation=dilation, rectified_conv=self.rectified_conv,\n",
        "                                rectify_avg=self.rectify_avg,\n",
        "                                norm_layer=norm_layer, dropblock_prob=dropblock_prob,\n",
        "                                last_gamma=self.last_gamma))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        if self.drop:\n",
        "            x = self.drop(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ajjFCmK8oEv"
      },
      "source": [
        "## Load pretrained ResNeSt Model on Cifar10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlBNmnt08fx0",
        "outputId": "39f51f8e-1f45-4939-c7bf-538395a68bda"
      },
      "source": [
        "cd /content/drive/MyDrive/CS3033 Final Project/results_hg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CS3033 Final Project/results_hg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSr9FeoW8nNq",
        "outputId": "94410a47-4c69-4d8c-8fbf-cca7ca2c81b2"
      },
      "source": [
        "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet(BasicBlock, [3, 4, 14, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,num_classes=10,\n",
        "                   deep_stem=True, stem_width=32, avg_down=True,\n",
        "                   avd=True, avd_first=False)\n",
        "model.load_state_dict(torch.load(\"resnest50_basic.pt\"))\n",
        "#map_location=torch.device('cpu')\n",
        "#model = model#.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNVP9lkz-X9X"
      },
      "source": [
        "## Import Cifar100 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNOnxn0b-Em2",
        "outputId": "22833d36-b532-43a8-a73f-68fc8696d16e"
      },
      "source": [
        "stats = ((0.5074,0.4867,0.4411),(0.2011,0.1987,0.2025))\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32,padding=4,padding_mode=\"reflect\"),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(*stats)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(*stats)\n",
        "    ]),\n",
        "}\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=data_transforms['train'])\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=data_transforms['val'])\n",
        "\n",
        "image_datasets = {'train':trainset, 'val':testset}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo_NVqzd-edN"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    cols       = ['epoch', 'test_acc']\n",
        "    results_df = pd.DataFrame(columns=cols).set_index('epoch')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "               phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val':\n",
        "                results_df.loc[epoch] = [epoch_acc.cpu().numpy()]\n",
        "                results_df.to_csv('/content/drive/MyDrive/CS3033 Final Project/results_hg/transfer_basic.csv')\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    #print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    #    time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ7pNmGg_N5e"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_ft = model\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 100)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 60 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=60, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA3tG8F-bujz"
      },
      "source": [
        "### Bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlKWoqUncHaj",
        "outputId": "f593bc3a-0468-4a6a-d8b1-de15a99ed7e9"
      },
      "source": [
        "model = ResNet(Bottleneck, [3, 4, 6, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,num_classes=100,\n",
        "                   deep_stem=True, stem_width=32, avg_down=True,\n",
        "                   avd=True, avd_first=False)\n",
        "model.load_state_dict(torch.load(\"resnest50_bottleneck_p100.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8OuGR-4_t-M",
        "outputId": "3784b77a-2122-4efb-beac-4611dbf56ef4"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/199\n",
            "----------\n",
            "train Loss: 4.3833 Acc: 0.0618\n",
            "val Loss: 4.0605 Acc: 0.1033\n",
            "Epoch 1/199\n",
            "----------\n",
            "train Loss: 3.8511 Acc: 0.1237\n",
            "val Loss: 3.5545 Acc: 0.1674\n",
            "Epoch 2/199\n",
            "----------\n",
            "train Loss: 3.4405 Acc: 0.1803\n",
            "val Loss: 3.1865 Acc: 0.2237\n",
            "Epoch 3/199\n",
            "----------\n",
            "train Loss: 3.1466 Acc: 0.2292\n",
            "val Loss: 2.9256 Acc: 0.2673\n",
            "Epoch 4/199\n",
            "----------\n",
            "train Loss: 2.9159 Acc: 0.2705\n",
            "val Loss: 2.7086 Acc: 0.3042\n",
            "Epoch 5/199\n",
            "----------\n",
            "train Loss: 2.7187 Acc: 0.3100\n",
            "val Loss: 2.5241 Acc: 0.3492\n",
            "Epoch 6/199\n",
            "----------\n",
            "train Loss: 2.5453 Acc: 0.3440\n",
            "val Loss: 2.3640 Acc: 0.3802\n",
            "Epoch 7/199\n",
            "----------\n",
            "train Loss: 2.4029 Acc: 0.3724\n",
            "val Loss: 2.2450 Acc: 0.4052\n",
            "Epoch 8/199\n",
            "----------\n",
            "train Loss: 2.2759 Acc: 0.4021\n",
            "val Loss: 2.1303 Acc: 0.4278\n",
            "Epoch 9/199\n",
            "----------\n",
            "train Loss: 2.1658 Acc: 0.4251\n",
            "val Loss: 2.0453 Acc: 0.4481\n",
            "Epoch 10/199\n",
            "----------\n",
            "train Loss: 2.0705 Acc: 0.4469\n",
            "val Loss: 1.9736 Acc: 0.4695\n",
            "Epoch 11/199\n",
            "----------\n",
            "train Loss: 1.9831 Acc: 0.4675\n",
            "val Loss: 1.9121 Acc: 0.4834\n",
            "Epoch 12/199\n",
            "----------\n",
            "train Loss: 1.9063 Acc: 0.4823\n",
            "val Loss: 1.8542 Acc: 0.4945\n",
            "Epoch 13/199\n",
            "----------\n",
            "train Loss: 1.8359 Acc: 0.5018\n",
            "val Loss: 1.8167 Acc: 0.5052\n",
            "Epoch 14/199\n",
            "----------\n",
            "train Loss: 1.7737 Acc: 0.5147\n",
            "val Loss: 1.7707 Acc: 0.5142\n",
            "Epoch 15/199\n",
            "----------\n",
            "train Loss: 1.7100 Acc: 0.5301\n",
            "val Loss: 1.7217 Acc: 0.5272\n",
            "Epoch 16/199\n",
            "----------\n",
            "train Loss: 1.6600 Acc: 0.5428\n",
            "val Loss: 1.7073 Acc: 0.5308\n",
            "Epoch 17/199\n",
            "----------\n",
            "train Loss: 1.6013 Acc: 0.5584\n",
            "val Loss: 1.6952 Acc: 0.5399\n",
            "Epoch 18/199\n",
            "----------\n",
            "train Loss: 1.5601 Acc: 0.5658\n",
            "val Loss: 1.6562 Acc: 0.5429\n",
            "Epoch 19/199\n",
            "----------\n",
            "train Loss: 1.5054 Acc: 0.5795\n",
            "val Loss: 1.6463 Acc: 0.5521\n",
            "Epoch 20/199\n",
            "----------\n",
            "train Loss: 1.4705 Acc: 0.5892\n",
            "val Loss: 1.6082 Acc: 0.5582\n",
            "Epoch 21/199\n",
            "----------\n",
            "train Loss: 1.4235 Acc: 0.6019\n",
            "val Loss: 1.6028 Acc: 0.5552\n",
            "Epoch 22/199\n",
            "----------\n",
            "train Loss: 1.3819 Acc: 0.6111\n",
            "val Loss: 1.6185 Acc: 0.5622\n",
            "Epoch 23/199\n",
            "----------\n",
            "train Loss: 1.3547 Acc: 0.6179\n",
            "val Loss: 1.5860 Acc: 0.5597\n",
            "Epoch 24/199\n",
            "----------\n",
            "train Loss: 1.3096 Acc: 0.6266\n",
            "val Loss: 1.5741 Acc: 0.5665\n",
            "Epoch 25/199\n",
            "----------\n",
            "train Loss: 1.2668 Acc: 0.6398\n",
            "val Loss: 1.5699 Acc: 0.5705\n",
            "Epoch 26/199\n",
            "----------\n",
            "train Loss: 1.2458 Acc: 0.6416\n",
            "val Loss: 1.5551 Acc: 0.5760\n",
            "Epoch 27/199\n",
            "----------\n",
            "train Loss: 1.2120 Acc: 0.6495\n",
            "val Loss: 1.5443 Acc: 0.5809\n",
            "Epoch 28/199\n",
            "----------\n",
            "train Loss: 1.1765 Acc: 0.6608\n",
            "val Loss: 1.5588 Acc: 0.5743\n",
            "Epoch 29/199\n",
            "----------\n",
            "train Loss: 1.1443 Acc: 0.6701\n",
            "val Loss: 1.5376 Acc: 0.5804\n",
            "Epoch 30/199\n",
            "----------\n",
            "train Loss: 1.1165 Acc: 0.6770\n",
            "val Loss: 1.5484 Acc: 0.5852\n",
            "Epoch 31/199\n",
            "----------\n",
            "train Loss: 1.0857 Acc: 0.6823\n",
            "val Loss: 1.5343 Acc: 0.5864\n",
            "Epoch 32/199\n",
            "----------\n",
            "train Loss: 1.0615 Acc: 0.6890\n",
            "val Loss: 1.5179 Acc: 0.5899\n",
            "Epoch 33/199\n",
            "----------\n",
            "train Loss: 1.0330 Acc: 0.6986\n",
            "val Loss: 1.5289 Acc: 0.5870\n",
            "Epoch 34/199\n",
            "----------\n",
            "train Loss: 1.0046 Acc: 0.7035\n",
            "val Loss: 1.5584 Acc: 0.5882\n",
            "Epoch 35/199\n",
            "----------\n",
            "train Loss: 0.9806 Acc: 0.7106\n",
            "val Loss: 1.5372 Acc: 0.5903\n",
            "Epoch 36/199\n",
            "----------\n",
            "train Loss: 0.9580 Acc: 0.7167\n",
            "val Loss: 1.5507 Acc: 0.5946\n",
            "Epoch 37/199\n",
            "----------\n",
            "train Loss: 0.9353 Acc: 0.7229\n",
            "val Loss: 1.5543 Acc: 0.5923\n",
            "Epoch 38/199\n",
            "----------\n",
            "train Loss: 0.9124 Acc: 0.7290\n",
            "val Loss: 1.5526 Acc: 0.5878\n",
            "Epoch 39/199\n",
            "----------\n",
            "train Loss: 0.8899 Acc: 0.7363\n",
            "val Loss: 1.5567 Acc: 0.5918\n",
            "Epoch 40/199\n",
            "----------\n",
            "train Loss: 0.8604 Acc: 0.7452\n",
            "val Loss: 1.5602 Acc: 0.5904\n",
            "Epoch 41/199\n",
            "----------\n",
            "train Loss: 0.8411 Acc: 0.7491\n",
            "val Loss: 1.5581 Acc: 0.5971\n",
            "Epoch 42/199\n",
            "----------\n",
            "train Loss: 0.8171 Acc: 0.7557\n",
            "val Loss: 1.5667 Acc: 0.5939\n",
            "Epoch 43/199\n",
            "----------\n",
            "train Loss: 0.7961 Acc: 0.7616\n",
            "val Loss: 1.5596 Acc: 0.5988\n",
            "Epoch 44/199\n",
            "----------\n",
            "train Loss: 0.7742 Acc: 0.7641\n",
            "val Loss: 1.5963 Acc: 0.5948\n",
            "Epoch 45/199\n",
            "----------\n",
            "train Loss: 0.7675 Acc: 0.7689\n",
            "val Loss: 1.5835 Acc: 0.5954\n",
            "Epoch 46/199\n",
            "----------\n",
            "train Loss: 0.7448 Acc: 0.7742\n",
            "val Loss: 1.5952 Acc: 0.6023\n",
            "Epoch 47/199\n",
            "----------\n",
            "train Loss: 0.7186 Acc: 0.7820\n",
            "val Loss: 1.5773 Acc: 0.5984\n",
            "Epoch 48/199\n",
            "----------\n",
            "train Loss: 0.6955 Acc: 0.7884\n",
            "val Loss: 1.5929 Acc: 0.5966\n",
            "Epoch 49/199\n",
            "----------\n",
            "train Loss: 0.6864 Acc: 0.7901\n",
            "val Loss: 1.5966 Acc: 0.5977\n",
            "Epoch 50/199\n",
            "----------\n",
            "train Loss: 0.6607 Acc: 0.7978\n",
            "val Loss: 1.5992 Acc: 0.5975\n",
            "Epoch 51/199\n",
            "----------\n",
            "train Loss: 0.6460 Acc: 0.8014\n",
            "val Loss: 1.6183 Acc: 0.6007\n",
            "Epoch 52/199\n",
            "----------\n",
            "train Loss: 0.6282 Acc: 0.8099\n",
            "val Loss: 1.6247 Acc: 0.6003\n",
            "Epoch 53/199\n",
            "----------\n",
            "train Loss: 0.6094 Acc: 0.8121\n",
            "val Loss: 1.6509 Acc: 0.6017\n",
            "Epoch 54/199\n",
            "----------\n",
            "train Loss: 0.6032 Acc: 0.8148\n",
            "val Loss: 1.6518 Acc: 0.5965\n",
            "Epoch 55/199\n",
            "----------\n",
            "train Loss: 0.5841 Acc: 0.8202\n",
            "val Loss: 1.6497 Acc: 0.5988\n",
            "Epoch 56/199\n",
            "----------\n",
            "train Loss: 0.5650 Acc: 0.8278\n",
            "val Loss: 1.6570 Acc: 0.6011\n",
            "Epoch 57/199\n",
            "----------\n",
            "train Loss: 0.5566 Acc: 0.8293\n",
            "val Loss: 1.6793 Acc: 0.6024\n",
            "Epoch 58/199\n",
            "----------\n",
            "train Loss: 0.5334 Acc: 0.8353\n",
            "val Loss: 1.6778 Acc: 0.6005\n",
            "Epoch 59/199\n",
            "----------\n",
            "train Loss: 0.5304 Acc: 0.8341\n",
            "val Loss: 1.6832 Acc: 0.5985\n",
            "Epoch 60/199\n",
            "----------\n",
            "train Loss: 0.4241 Acc: 0.8710\n",
            "val Loss: 1.6009 Acc: 0.6170\n",
            "Epoch 61/199\n",
            "----------\n",
            "train Loss: 0.3838 Acc: 0.8847\n",
            "val Loss: 1.6066 Acc: 0.6184\n",
            "Epoch 62/199\n",
            "----------\n",
            "train Loss: 0.3608 Acc: 0.8918\n",
            "val Loss: 1.6090 Acc: 0.6191\n",
            "Epoch 63/199\n",
            "----------\n",
            "train Loss: 0.3533 Acc: 0.8961\n",
            "val Loss: 1.6075 Acc: 0.6176\n",
            "Epoch 64/199\n",
            "----------\n",
            "train Loss: 0.3405 Acc: 0.8989\n",
            "val Loss: 1.6121 Acc: 0.6190\n",
            "Epoch 65/199\n",
            "----------\n",
            "train Loss: 0.3388 Acc: 0.9012\n",
            "val Loss: 1.6275 Acc: 0.6192\n",
            "Epoch 66/199\n",
            "----------\n",
            "train Loss: 0.3248 Acc: 0.9043\n",
            "val Loss: 1.6186 Acc: 0.6189\n",
            "Epoch 67/199\n",
            "----------\n",
            "train Loss: 0.3222 Acc: 0.9051\n",
            "val Loss: 1.6413 Acc: 0.6171\n",
            "Epoch 68/199\n",
            "----------\n",
            "train Loss: 0.3172 Acc: 0.9071\n",
            "val Loss: 1.6379 Acc: 0.6179\n",
            "Epoch 69/199\n",
            "----------\n",
            "train Loss: 0.3116 Acc: 0.9089\n",
            "val Loss: 1.6458 Acc: 0.6153\n",
            "Epoch 70/199\n",
            "----------\n",
            "train Loss: 0.3090 Acc: 0.9098\n",
            "val Loss: 1.6438 Acc: 0.6204\n",
            "Epoch 71/199\n",
            "----------\n",
            "train Loss: 0.3125 Acc: 0.9070\n",
            "val Loss: 1.6428 Acc: 0.6172\n",
            "Epoch 72/199\n",
            "----------\n",
            "train Loss: 0.2985 Acc: 0.9135\n",
            "val Loss: 1.6419 Acc: 0.6193\n",
            "Epoch 73/199\n",
            "----------\n",
            "train Loss: 0.2956 Acc: 0.9143\n",
            "val Loss: 1.6517 Acc: 0.6179\n",
            "Epoch 74/199\n",
            "----------\n",
            "train Loss: 0.2922 Acc: 0.9146\n",
            "val Loss: 1.6675 Acc: 0.6154\n",
            "Epoch 75/199\n",
            "----------\n",
            "train Loss: 0.2896 Acc: 0.9143\n",
            "val Loss: 1.6550 Acc: 0.6159\n",
            "Epoch 76/199\n",
            "----------\n",
            "train Loss: 0.2867 Acc: 0.9159\n",
            "val Loss: 1.6677 Acc: 0.6171\n",
            "Epoch 77/199\n",
            "----------\n",
            "train Loss: 0.2811 Acc: 0.9188\n",
            "val Loss: 1.6711 Acc: 0.6154\n",
            "Epoch 78/199\n",
            "----------\n",
            "train Loss: 0.2774 Acc: 0.9189\n",
            "val Loss: 1.6772 Acc: 0.6136\n",
            "Epoch 79/199\n",
            "----------\n",
            "train Loss: 0.2747 Acc: 0.9197\n",
            "val Loss: 1.6649 Acc: 0.6175\n",
            "Epoch 80/199\n",
            "----------\n",
            "train Loss: 0.2761 Acc: 0.9194\n",
            "val Loss: 1.6807 Acc: 0.6134\n",
            "Epoch 81/199\n",
            "----------\n",
            "train Loss: 0.2643 Acc: 0.9238\n",
            "val Loss: 1.6792 Acc: 0.6163\n",
            "Epoch 82/199\n",
            "----------\n",
            "train Loss: 0.2657 Acc: 0.9234\n",
            "val Loss: 1.6645 Acc: 0.6202\n",
            "Epoch 83/199\n",
            "----------\n",
            "train Loss: 0.2645 Acc: 0.9228\n",
            "val Loss: 1.6926 Acc: 0.6170\n",
            "Epoch 84/199\n",
            "----------\n",
            "train Loss: 0.2641 Acc: 0.9241\n",
            "val Loss: 1.6941 Acc: 0.6131\n",
            "Epoch 85/199\n",
            "----------\n",
            "train Loss: 0.2600 Acc: 0.9251\n",
            "val Loss: 1.6940 Acc: 0.6160\n",
            "Epoch 86/199\n",
            "----------\n",
            "train Loss: 0.2567 Acc: 0.9256\n",
            "val Loss: 1.6847 Acc: 0.6181\n",
            "Epoch 87/199\n",
            "----------\n",
            "train Loss: 0.2551 Acc: 0.9268\n",
            "val Loss: 1.6931 Acc: 0.6138\n",
            "Epoch 88/199\n",
            "----------\n",
            "train Loss: 0.2499 Acc: 0.9274\n",
            "val Loss: 1.6906 Acc: 0.6173\n",
            "Epoch 89/199\n",
            "----------\n",
            "train Loss: 0.2489 Acc: 0.9276\n",
            "val Loss: 1.6835 Acc: 0.6164\n",
            "Epoch 90/199\n",
            "----------\n",
            "train Loss: 0.2483 Acc: 0.9284\n",
            "val Loss: 1.6992 Acc: 0.6171\n",
            "Epoch 91/199\n",
            "----------\n",
            "train Loss: 0.2482 Acc: 0.9293\n",
            "val Loss: 1.6969 Acc: 0.6161\n",
            "Epoch 92/199\n",
            "----------\n",
            "train Loss: 0.2434 Acc: 0.9305\n",
            "val Loss: 1.7189 Acc: 0.6141\n",
            "Epoch 93/199\n",
            "----------\n",
            "train Loss: 0.2386 Acc: 0.9310\n",
            "val Loss: 1.7113 Acc: 0.6159\n",
            "Epoch 94/199\n",
            "----------\n",
            "train Loss: 0.2396 Acc: 0.9310\n",
            "val Loss: 1.7112 Acc: 0.6159\n",
            "Epoch 95/199\n",
            "----------\n",
            "train Loss: 0.2401 Acc: 0.9311\n",
            "val Loss: 1.7255 Acc: 0.6148\n",
            "Epoch 96/199\n",
            "----------\n",
            "train Loss: 0.2338 Acc: 0.9328\n",
            "val Loss: 1.7217 Acc: 0.6156\n",
            "Epoch 97/199\n",
            "----------\n",
            "train Loss: 0.2316 Acc: 0.9345\n",
            "val Loss: 1.7338 Acc: 0.6155\n",
            "Epoch 98/199\n",
            "----------\n",
            "train Loss: 0.2296 Acc: 0.9345\n",
            "val Loss: 1.7277 Acc: 0.6165\n",
            "Epoch 99/199\n",
            "----------\n",
            "train Loss: 0.2235 Acc: 0.9364\n",
            "val Loss: 1.7378 Acc: 0.6144\n",
            "Epoch 100/199\n",
            "----------\n",
            "train Loss: 0.2257 Acc: 0.9354\n",
            "val Loss: 1.7281 Acc: 0.6163\n",
            "Epoch 101/199\n",
            "----------\n",
            "train Loss: 0.2195 Acc: 0.9379\n",
            "val Loss: 1.7514 Acc: 0.6146\n",
            "Epoch 102/199\n",
            "----------\n",
            "train Loss: 0.2223 Acc: 0.9363\n",
            "val Loss: 1.7373 Acc: 0.6145\n",
            "Epoch 103/199\n",
            "----------\n",
            "train Loss: 0.2194 Acc: 0.9370\n",
            "val Loss: 1.7277 Acc: 0.6182\n",
            "Epoch 104/199\n",
            "----------\n",
            "train Loss: 0.2148 Acc: 0.9392\n",
            "val Loss: 1.7417 Acc: 0.6137\n",
            "Epoch 105/199\n",
            "----------\n",
            "train Loss: 0.2116 Acc: 0.9396\n",
            "val Loss: 1.7351 Acc: 0.6170\n",
            "Epoch 106/199\n",
            "----------\n",
            "train Loss: 0.2119 Acc: 0.9416\n",
            "val Loss: 1.7623 Acc: 0.6136\n",
            "Epoch 107/199\n",
            "----------\n",
            "train Loss: 0.2154 Acc: 0.9378\n",
            "val Loss: 1.7497 Acc: 0.6155\n",
            "Epoch 108/199\n",
            "----------\n",
            "train Loss: 0.2115 Acc: 0.9391\n",
            "val Loss: 1.7465 Acc: 0.6157\n",
            "Epoch 109/199\n",
            "----------\n",
            "train Loss: 0.2031 Acc: 0.9431\n",
            "val Loss: 1.7559 Acc: 0.6130\n",
            "Epoch 110/199\n",
            "----------\n",
            "train Loss: 0.2072 Acc: 0.9405\n",
            "val Loss: 1.7508 Acc: 0.6163\n",
            "Epoch 111/199\n",
            "----------\n",
            "train Loss: 0.2059 Acc: 0.9412\n",
            "val Loss: 1.7597 Acc: 0.6151\n",
            "Epoch 112/199\n",
            "----------\n",
            "train Loss: 0.2021 Acc: 0.9426\n",
            "val Loss: 1.7529 Acc: 0.6162\n",
            "Epoch 113/199\n",
            "----------\n",
            "train Loss: 0.2013 Acc: 0.9420\n",
            "val Loss: 1.7690 Acc: 0.6143\n",
            "Epoch 114/199\n",
            "----------\n",
            "train Loss: 0.1993 Acc: 0.9428\n",
            "val Loss: 1.7676 Acc: 0.6132\n",
            "Epoch 115/199\n",
            "----------\n",
            "train Loss: 0.1985 Acc: 0.9437\n",
            "val Loss: 1.7529 Acc: 0.6176\n",
            "Epoch 116/199\n",
            "----------\n",
            "train Loss: 0.1953 Acc: 0.9457\n",
            "val Loss: 1.7688 Acc: 0.6164\n",
            "Epoch 117/199\n",
            "----------\n",
            "train Loss: 0.1944 Acc: 0.9452\n",
            "val Loss: 1.7853 Acc: 0.6146\n",
            "Epoch 118/199\n",
            "----------\n",
            "train Loss: 0.1924 Acc: 0.9449\n",
            "val Loss: 1.7830 Acc: 0.6137\n",
            "Epoch 119/199\n",
            "----------\n",
            "train Loss: 0.1934 Acc: 0.9447\n",
            "val Loss: 1.7855 Acc: 0.6147\n",
            "Epoch 120/199\n",
            "----------\n",
            "train Loss: 0.1823 Acc: 0.9496\n",
            "val Loss: 1.7737 Acc: 0.6180\n",
            "Epoch 121/199\n",
            "----------\n",
            "train Loss: 0.1800 Acc: 0.9496\n",
            "val Loss: 1.7859 Acc: 0.6143\n",
            "Epoch 122/199\n",
            "----------\n",
            "train Loss: 0.1773 Acc: 0.9519\n",
            "val Loss: 1.7820 Acc: 0.6172\n",
            "Epoch 123/199\n",
            "----------\n",
            "train Loss: 0.1798 Acc: 0.9499\n",
            "val Loss: 1.7812 Acc: 0.6166\n",
            "Epoch 124/199\n",
            "----------\n",
            "train Loss: 0.1812 Acc: 0.9502\n",
            "val Loss: 1.7635 Acc: 0.6160\n",
            "Epoch 125/199\n",
            "----------\n",
            "train Loss: 0.1779 Acc: 0.9508\n",
            "val Loss: 1.7717 Acc: 0.6154\n",
            "Epoch 126/199\n",
            "----------\n",
            "train Loss: 0.1815 Acc: 0.9491\n",
            "val Loss: 1.8023 Acc: 0.6139\n",
            "Epoch 127/199\n",
            "----------\n",
            "train Loss: 0.1770 Acc: 0.9516\n",
            "val Loss: 1.7870 Acc: 0.6160\n",
            "Epoch 128/199\n",
            "----------\n",
            "train Loss: 0.1759 Acc: 0.9507\n",
            "val Loss: 1.7814 Acc: 0.6167\n",
            "Epoch 129/199\n",
            "----------\n",
            "train Loss: 0.1754 Acc: 0.9512\n",
            "val Loss: 1.7795 Acc: 0.6157\n",
            "Epoch 130/199\n",
            "----------\n",
            "train Loss: 0.1800 Acc: 0.9498\n",
            "val Loss: 1.7801 Acc: 0.6162\n",
            "Epoch 131/199\n",
            "----------\n",
            "train Loss: 0.1770 Acc: 0.9515\n",
            "val Loss: 1.7832 Acc: 0.6132\n",
            "Epoch 132/199\n",
            "----------\n",
            "train Loss: 0.1777 Acc: 0.9497\n",
            "val Loss: 1.7727 Acc: 0.6161\n",
            "Epoch 133/199\n",
            "----------\n",
            "train Loss: 0.1739 Acc: 0.9521\n",
            "val Loss: 1.7781 Acc: 0.6175\n",
            "Epoch 134/199\n",
            "----------\n",
            "train Loss: 0.1754 Acc: 0.9520\n",
            "val Loss: 1.7791 Acc: 0.6144\n",
            "Epoch 135/199\n",
            "----------\n",
            "train Loss: 0.1722 Acc: 0.9530\n",
            "val Loss: 1.7921 Acc: 0.6156\n",
            "Epoch 136/199\n",
            "----------\n",
            "train Loss: 0.1738 Acc: 0.9512\n",
            "val Loss: 1.7863 Acc: 0.6158\n",
            "Epoch 137/199\n",
            "----------\n",
            "train Loss: 0.1707 Acc: 0.9535\n",
            "val Loss: 1.7963 Acc: 0.6147\n",
            "Epoch 138/199\n",
            "----------\n",
            "train Loss: 0.1721 Acc: 0.9522\n",
            "val Loss: 1.7674 Acc: 0.6193\n",
            "Epoch 139/199\n",
            "----------\n",
            "train Loss: 0.1748 Acc: 0.9515\n",
            "val Loss: 1.7778 Acc: 0.6154\n",
            "Epoch 140/199\n",
            "----------\n",
            "train Loss: 0.1688 Acc: 0.9532\n",
            "val Loss: 1.7749 Acc: 0.6206\n",
            "Epoch 141/199\n",
            "----------\n",
            "train Loss: 0.1755 Acc: 0.9514\n",
            "val Loss: 1.7706 Acc: 0.6163\n",
            "Epoch 142/199\n",
            "----------\n",
            "train Loss: 0.1687 Acc: 0.9539\n",
            "val Loss: 1.7780 Acc: 0.6158\n",
            "Epoch 143/199\n",
            "----------\n",
            "train Loss: 0.1669 Acc: 0.9535\n",
            "val Loss: 1.7759 Acc: 0.6162\n",
            "Epoch 144/199\n",
            "----------\n",
            "train Loss: 0.1703 Acc: 0.9541\n",
            "val Loss: 1.7856 Acc: 0.6151\n",
            "Epoch 145/199\n",
            "----------\n",
            "train Loss: 0.1692 Acc: 0.9534\n",
            "val Loss: 1.7898 Acc: 0.6164\n",
            "Epoch 146/199\n",
            "----------\n",
            "train Loss: 0.1705 Acc: 0.9522\n",
            "val Loss: 1.7805 Acc: 0.6168\n",
            "Epoch 147/199\n",
            "----------\n",
            "train Loss: 0.1723 Acc: 0.9531\n",
            "val Loss: 1.7794 Acc: 0.6169\n",
            "Epoch 148/199\n",
            "----------\n",
            "train Loss: 0.1710 Acc: 0.9530\n",
            "val Loss: 1.7777 Acc: 0.6155\n",
            "Epoch 149/199\n",
            "----------\n",
            "train Loss: 0.1740 Acc: 0.9514\n",
            "val Loss: 1.7760 Acc: 0.6166\n",
            "Epoch 150/199\n",
            "----------\n",
            "train Loss: 0.1720 Acc: 0.9513\n",
            "val Loss: 1.7824 Acc: 0.6158\n",
            "Epoch 151/199\n",
            "----------\n",
            "train Loss: 0.1700 Acc: 0.9525\n",
            "val Loss: 1.7822 Acc: 0.6156\n",
            "Epoch 152/199\n",
            "----------\n",
            "train Loss: 0.1711 Acc: 0.9520\n",
            "val Loss: 1.7896 Acc: 0.6164\n",
            "Epoch 153/199\n",
            "----------\n",
            "train Loss: 0.1700 Acc: 0.9537\n",
            "val Loss: 1.7866 Acc: 0.6145\n",
            "Epoch 154/199\n",
            "----------\n",
            "train Loss: 0.1663 Acc: 0.9554\n",
            "val Loss: 1.7818 Acc: 0.6149\n",
            "Epoch 155/199\n",
            "----------\n",
            "train Loss: 0.1695 Acc: 0.9541\n",
            "val Loss: 1.7845 Acc: 0.6153\n",
            "Epoch 156/199\n",
            "----------\n",
            "train Loss: 0.1708 Acc: 0.9530\n",
            "val Loss: 1.7822 Acc: 0.6151\n",
            "Epoch 157/199\n",
            "----------\n",
            "train Loss: 0.1667 Acc: 0.9533\n",
            "val Loss: 1.7884 Acc: 0.6151\n",
            "Epoch 158/199\n",
            "----------\n",
            "train Loss: 0.1663 Acc: 0.9544\n",
            "val Loss: 1.7724 Acc: 0.6170\n",
            "Epoch 159/199\n",
            "----------\n",
            "train Loss: 0.1662 Acc: 0.9548\n",
            "val Loss: 1.7897 Acc: 0.6140\n",
            "Epoch 160/199\n",
            "----------\n",
            "train Loss: 0.1660 Acc: 0.9547\n",
            "val Loss: 1.7880 Acc: 0.6169\n",
            "Epoch 161/199\n",
            "----------\n",
            "train Loss: 0.1686 Acc: 0.9526\n",
            "val Loss: 1.7960 Acc: 0.6157\n",
            "Epoch 162/199\n",
            "----------\n",
            "train Loss: 0.1636 Acc: 0.9551\n",
            "val Loss: 1.7807 Acc: 0.6175\n",
            "Epoch 163/199\n",
            "----------\n",
            "train Loss: 0.1673 Acc: 0.9535\n",
            "val Loss: 1.7907 Acc: 0.6141\n",
            "Epoch 164/199\n",
            "----------\n",
            "train Loss: 0.1669 Acc: 0.9542\n",
            "val Loss: 1.7878 Acc: 0.6136\n",
            "Epoch 165/199\n",
            "----------\n",
            "train Loss: 0.1663 Acc: 0.9543\n",
            "val Loss: 1.7919 Acc: 0.6137\n",
            "Epoch 166/199\n",
            "----------\n",
            "train Loss: 0.1678 Acc: 0.9534\n",
            "val Loss: 1.7907 Acc: 0.6160\n",
            "Epoch 167/199\n",
            "----------\n",
            "train Loss: 0.1671 Acc: 0.9544\n",
            "val Loss: 1.7866 Acc: 0.6146\n",
            "Epoch 168/199\n",
            "----------\n",
            "train Loss: 0.1678 Acc: 0.9536\n",
            "val Loss: 1.7838 Acc: 0.6165\n",
            "Epoch 169/199\n",
            "----------\n",
            "train Loss: 0.1633 Acc: 0.9554\n",
            "val Loss: 1.7857 Acc: 0.6173\n",
            "Epoch 170/199\n",
            "----------\n",
            "train Loss: 0.1605 Acc: 0.9556\n",
            "val Loss: 1.8043 Acc: 0.6142\n",
            "Epoch 171/199\n",
            "----------\n",
            "train Loss: 0.1666 Acc: 0.9545\n",
            "val Loss: 1.7903 Acc: 0.6163\n",
            "Epoch 172/199\n",
            "----------\n",
            "train Loss: 0.1649 Acc: 0.9545\n",
            "val Loss: 1.7893 Acc: 0.6154\n",
            "Epoch 173/199\n",
            "----------\n",
            "train Loss: 0.1662 Acc: 0.9536\n",
            "val Loss: 1.7943 Acc: 0.6164\n",
            "Epoch 174/199\n",
            "----------\n",
            "train Loss: 0.1647 Acc: 0.9545\n",
            "val Loss: 1.7906 Acc: 0.6150\n",
            "Epoch 175/199\n",
            "----------\n",
            "train Loss: 0.1623 Acc: 0.9564\n",
            "val Loss: 1.7913 Acc: 0.6176\n",
            "Epoch 176/199\n",
            "----------\n",
            "train Loss: 0.1648 Acc: 0.9551\n",
            "val Loss: 1.7806 Acc: 0.6181\n",
            "Epoch 177/199\n",
            "----------\n",
            "train Loss: 0.1640 Acc: 0.9550\n",
            "val Loss: 1.8086 Acc: 0.6143\n",
            "Epoch 178/199\n",
            "----------\n",
            "train Loss: 0.1645 Acc: 0.9542\n",
            "val Loss: 1.7970 Acc: 0.6147\n",
            "Epoch 179/199\n",
            "----------\n",
            "train Loss: 0.1597 Acc: 0.9563\n",
            "val Loss: 1.7942 Acc: 0.6128\n",
            "Epoch 180/199\n",
            "----------\n",
            "train Loss: 0.1605 Acc: 0.9566\n",
            "val Loss: 1.8063 Acc: 0.6128\n",
            "Epoch 181/199\n",
            "----------\n",
            "train Loss: 0.1616 Acc: 0.9557\n",
            "val Loss: 1.8025 Acc: 0.6147\n",
            "Epoch 182/199\n",
            "----------\n",
            "train Loss: 0.1620 Acc: 0.9548\n",
            "val Loss: 1.7870 Acc: 0.6167\n",
            "Epoch 183/199\n",
            "----------\n",
            "train Loss: 0.1620 Acc: 0.9556\n",
            "val Loss: 1.7828 Acc: 0.6161\n",
            "Epoch 184/199\n",
            "----------\n",
            "train Loss: 0.1615 Acc: 0.9556\n",
            "val Loss: 1.8067 Acc: 0.6130\n",
            "Epoch 185/199\n",
            "----------\n",
            "train Loss: 0.1604 Acc: 0.9564\n",
            "val Loss: 1.8076 Acc: 0.6134\n",
            "Epoch 186/199\n",
            "----------\n",
            "train Loss: 0.1628 Acc: 0.9549\n",
            "val Loss: 1.8077 Acc: 0.6137\n",
            "Epoch 187/199\n",
            "----------\n",
            "train Loss: 0.1609 Acc: 0.9566\n",
            "val Loss: 1.7976 Acc: 0.6173\n",
            "Epoch 188/199\n",
            "----------\n",
            "train Loss: 0.1635 Acc: 0.9551\n",
            "val Loss: 1.7863 Acc: 0.6180\n",
            "Epoch 189/199\n",
            "----------\n",
            "train Loss: 0.1608 Acc: 0.9565\n",
            "val Loss: 1.7936 Acc: 0.6167\n",
            "Epoch 190/199\n",
            "----------\n",
            "train Loss: 0.1629 Acc: 0.9555\n",
            "val Loss: 1.7952 Acc: 0.6164\n",
            "Epoch 191/199\n",
            "----------\n",
            "train Loss: 0.1657 Acc: 0.9540\n",
            "val Loss: 1.7960 Acc: 0.6163\n",
            "Epoch 192/199\n",
            "----------\n",
            "train Loss: 0.1610 Acc: 0.9556\n",
            "val Loss: 1.7988 Acc: 0.6131\n",
            "Epoch 193/199\n",
            "----------\n",
            "train Loss: 0.1588 Acc: 0.9566\n",
            "val Loss: 1.7899 Acc: 0.6169\n",
            "Epoch 194/199\n",
            "----------\n",
            "train Loss: 0.1631 Acc: 0.9551\n",
            "val Loss: 1.7899 Acc: 0.6158\n",
            "Epoch 195/199\n",
            "----------\n",
            "train Loss: 0.1618 Acc: 0.9550\n",
            "val Loss: 1.7847 Acc: 0.6163\n",
            "Epoch 196/199\n",
            "----------\n",
            "train Loss: 0.1608 Acc: 0.9563\n",
            "val Loss: 1.7895 Acc: 0.6174\n",
            "Epoch 197/199\n",
            "----------\n",
            "train Loss: 0.1618 Acc: 0.9558\n",
            "val Loss: 1.8044 Acc: 0.6147\n",
            "Epoch 198/199\n",
            "----------\n",
            "train Loss: 0.1620 Acc: 0.9560\n",
            "val Loss: 1.7931 Acc: 0.6158\n",
            "Epoch 199/199\n",
            "----------\n",
            "train Loss: 0.1613 Acc: 0.9556\n",
            "val Loss: 1.7894 Acc: 0.6152\n",
            "Best val Acc: 0.620600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tttH42d7bw78"
      },
      "source": [
        "### Basic block\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-3ywo30cb7q",
        "outputId": "f593bc3a-0468-4a6a-d8b1-de15a99ed7e9"
      },
      "source": [
        "model = ResNet(BasicBlock, [3, 4, 14, 3],\n",
        "                   radix=2, groups=1, bottleneck_width=64,num_classes=10,\n",
        "                   deep_stem=True, stem_width=32, avg_down=True,\n",
        "                   avd=True, avd_first=False)\n",
        "model.load_state_dict(torch.load(\"resnest50_basic.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y26VcJ1C7WjL",
        "outputId": "82a16c93-b764-4413-bb34-a693d6d439c9"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/199\n",
            "----------\n",
            "train Loss: 4.1204 Acc: 0.1299\n",
            "val Loss: 3.5925 Acc: 0.2090\n",
            "Epoch 1/199\n",
            "----------\n",
            "train Loss: 3.3116 Acc: 0.2433\n",
            "val Loss: 2.9301 Acc: 0.3008\n",
            "Epoch 2/199\n",
            "----------\n",
            "train Loss: 2.8207 Acc: 0.3151\n",
            "val Loss: 2.5178 Acc: 0.3681\n",
            "Epoch 3/199\n",
            "----------\n",
            "train Loss: 2.5054 Acc: 0.3662\n",
            "val Loss: 2.2673 Acc: 0.4048\n",
            "Epoch 4/199\n",
            "----------\n",
            "train Loss: 2.2811 Acc: 0.4051\n",
            "val Loss: 2.0751 Acc: 0.4436\n",
            "Epoch 5/199\n",
            "----------\n",
            "train Loss: 2.1073 Acc: 0.4404\n",
            "val Loss: 1.9449 Acc: 0.4736\n",
            "Epoch 6/199\n",
            "----------\n",
            "train Loss: 1.9690 Acc: 0.4703\n",
            "val Loss: 1.8485 Acc: 0.5000\n",
            "Epoch 7/199\n",
            "----------\n",
            "train Loss: 1.8533 Acc: 0.4956\n",
            "val Loss: 1.7702 Acc: 0.5148\n",
            "Epoch 8/199\n",
            "----------\n",
            "train Loss: 1.7514 Acc: 0.5204\n",
            "val Loss: 1.6932 Acc: 0.5279\n",
            "Epoch 9/199\n",
            "----------\n",
            "train Loss: 1.6575 Acc: 0.5428\n",
            "val Loss: 1.6610 Acc: 0.5390\n",
            "Epoch 10/199\n",
            "----------\n",
            "train Loss: 1.5654 Acc: 0.5632\n",
            "val Loss: 1.6120 Acc: 0.5511\n",
            "Epoch 11/199\n",
            "----------\n",
            "train Loss: 1.4829 Acc: 0.5851\n",
            "val Loss: 1.5867 Acc: 0.5631\n",
            "Epoch 12/199\n",
            "----------\n",
            "train Loss: 1.4099 Acc: 0.6012\n",
            "val Loss: 1.5615 Acc: 0.5674\n",
            "Epoch 13/199\n",
            "----------\n",
            "train Loss: 1.3369 Acc: 0.6197\n",
            "val Loss: 1.5376 Acc: 0.5768\n",
            "Epoch 14/199\n",
            "----------\n",
            "train Loss: 1.2624 Acc: 0.6375\n",
            "val Loss: 1.5188 Acc: 0.5821\n",
            "Epoch 15/199\n",
            "----------\n",
            "train Loss: 1.1928 Acc: 0.6565\n",
            "val Loss: 1.5165 Acc: 0.5850\n",
            "Epoch 16/199\n",
            "----------\n",
            "train Loss: 1.1357 Acc: 0.6738\n",
            "val Loss: 1.5173 Acc: 0.5876\n",
            "Epoch 17/199\n",
            "----------\n",
            "train Loss: 1.0742 Acc: 0.6898\n",
            "val Loss: 1.5126 Acc: 0.5892\n",
            "Epoch 18/199\n",
            "----------\n",
            "train Loss: 1.0079 Acc: 0.7057\n",
            "val Loss: 1.5039 Acc: 0.5930\n",
            "Epoch 19/199\n",
            "----------\n",
            "train Loss: 0.9549 Acc: 0.7178\n",
            "val Loss: 1.5035 Acc: 0.5970\n",
            "Epoch 20/199\n",
            "----------\n",
            "train Loss: 0.9007 Acc: 0.7326\n",
            "val Loss: 1.5207 Acc: 0.6008\n",
            "Epoch 21/199\n",
            "----------\n",
            "train Loss: 0.8466 Acc: 0.7476\n",
            "val Loss: 1.5323 Acc: 0.5975\n",
            "Epoch 22/199\n",
            "----------\n",
            "train Loss: 0.7960 Acc: 0.7617\n",
            "val Loss: 1.5396 Acc: 0.5984\n",
            "Epoch 23/199\n",
            "----------\n",
            "train Loss: 0.7520 Acc: 0.7760\n",
            "val Loss: 1.5418 Acc: 0.6009\n",
            "Epoch 24/199\n",
            "----------\n",
            "train Loss: 0.7065 Acc: 0.7874\n",
            "val Loss: 1.5563 Acc: 0.5993\n",
            "Epoch 25/199\n",
            "----------\n",
            "train Loss: 0.6579 Acc: 0.8027\n",
            "val Loss: 1.5879 Acc: 0.5967\n",
            "Epoch 26/199\n",
            "----------\n",
            "train Loss: 0.6217 Acc: 0.8114\n",
            "val Loss: 1.5692 Acc: 0.6082\n",
            "Epoch 27/199\n",
            "----------\n",
            "train Loss: 0.5824 Acc: 0.8225\n",
            "val Loss: 1.5865 Acc: 0.6057\n",
            "Epoch 28/199\n",
            "----------\n",
            "train Loss: 0.5415 Acc: 0.8363\n",
            "val Loss: 1.6012 Acc: 0.6037\n",
            "Epoch 29/199\n",
            "----------\n",
            "train Loss: 0.5180 Acc: 0.8405\n",
            "val Loss: 1.6146 Acc: 0.6043\n",
            "Epoch 30/199\n",
            "----------\n",
            "train Loss: 0.4788 Acc: 0.8525\n",
            "val Loss: 1.6263 Acc: 0.6002\n",
            "Epoch 31/199\n",
            "----------\n",
            "train Loss: 0.4515 Acc: 0.8633\n",
            "val Loss: 1.6511 Acc: 0.6053\n",
            "Epoch 32/199\n",
            "----------\n",
            "train Loss: 0.4241 Acc: 0.8714\n",
            "val Loss: 1.6573 Acc: 0.6037\n",
            "Epoch 33/199\n",
            "----------\n",
            "train Loss: 0.3890 Acc: 0.8811\n",
            "val Loss: 1.6954 Acc: 0.6022\n",
            "Epoch 34/199\n",
            "----------\n",
            "train Loss: 0.3659 Acc: 0.8863\n",
            "val Loss: 1.6972 Acc: 0.6057\n",
            "Epoch 35/199\n",
            "----------\n",
            "train Loss: 0.3456 Acc: 0.8942\n",
            "val Loss: 1.6948 Acc: 0.6050\n",
            "Epoch 36/199\n",
            "----------\n",
            "train Loss: 0.3247 Acc: 0.9014\n",
            "val Loss: 1.7376 Acc: 0.6015\n",
            "Epoch 37/199\n",
            "----------\n",
            "train Loss: 0.3078 Acc: 0.9058\n",
            "val Loss: 1.7734 Acc: 0.6030\n",
            "Epoch 38/199\n",
            "----------\n",
            "train Loss: 0.2904 Acc: 0.9091\n",
            "val Loss: 1.7485 Acc: 0.6043\n",
            "Epoch 39/199\n",
            "----------\n",
            "train Loss: 0.2694 Acc: 0.9184\n",
            "val Loss: 1.7727 Acc: 0.6037\n",
            "Epoch 40/199\n",
            "----------\n",
            "train Loss: 0.2534 Acc: 0.9229\n",
            "val Loss: 1.7923 Acc: 0.6043\n",
            "Epoch 41/199\n",
            "----------\n",
            "train Loss: 0.2410 Acc: 0.9259\n",
            "val Loss: 1.8092 Acc: 0.6046\n",
            "Epoch 42/199\n",
            "----------\n",
            "train Loss: 0.2341 Acc: 0.9278\n",
            "val Loss: 1.8095 Acc: 0.6044\n",
            "Epoch 43/199\n",
            "----------\n",
            "train Loss: 0.2174 Acc: 0.9341\n",
            "val Loss: 1.8055 Acc: 0.6060\n",
            "Epoch 44/199\n",
            "----------\n",
            "train Loss: 0.2075 Acc: 0.9365\n",
            "val Loss: 1.8483 Acc: 0.6033\n",
            "Epoch 45/199\n",
            "----------\n",
            "train Loss: 0.2023 Acc: 0.9381\n",
            "val Loss: 1.8223 Acc: 0.6047\n",
            "Epoch 46/199\n",
            "----------\n",
            "train Loss: 0.1904 Acc: 0.9420\n",
            "val Loss: 1.8396 Acc: 0.6138\n",
            "Epoch 47/199\n",
            "----------\n",
            "train Loss: 0.1820 Acc: 0.9450\n",
            "val Loss: 1.8424 Acc: 0.6127\n",
            "Epoch 48/199\n",
            "----------\n",
            "train Loss: 0.1683 Acc: 0.9488\n",
            "val Loss: 1.8563 Acc: 0.6101\n",
            "Epoch 49/199\n",
            "----------\n",
            "train Loss: 0.1560 Acc: 0.9529\n",
            "val Loss: 1.8796 Acc: 0.6087\n",
            "Epoch 50/199\n",
            "----------\n",
            "train Loss: 0.1542 Acc: 0.9530\n",
            "val Loss: 1.8762 Acc: 0.6101\n",
            "Epoch 51/199\n",
            "----------\n",
            "train Loss: 0.1483 Acc: 0.9549\n",
            "val Loss: 1.8895 Acc: 0.6074\n",
            "Epoch 52/199\n",
            "----------\n",
            "train Loss: 0.1370 Acc: 0.9595\n",
            "val Loss: 1.8889 Acc: 0.6081\n",
            "Epoch 53/199\n",
            "----------\n",
            "train Loss: 0.1406 Acc: 0.9571\n",
            "val Loss: 1.9209 Acc: 0.6041\n",
            "Epoch 54/199\n",
            "----------\n",
            "train Loss: 0.1334 Acc: 0.9606\n",
            "val Loss: 1.9160 Acc: 0.6115\n",
            "Epoch 55/199\n",
            "----------\n",
            "train Loss: 0.1226 Acc: 0.9640\n",
            "val Loss: 1.8814 Acc: 0.6105\n",
            "Epoch 56/199\n",
            "----------\n",
            "train Loss: 0.1180 Acc: 0.9648\n",
            "val Loss: 1.9139 Acc: 0.6097\n",
            "Epoch 57/199\n",
            "----------\n",
            "train Loss: 0.1156 Acc: 0.9655\n",
            "val Loss: 1.9217 Acc: 0.6131\n",
            "Epoch 58/199\n",
            "----------\n",
            "train Loss: 0.1111 Acc: 0.9672\n",
            "val Loss: 1.9041 Acc: 0.6147\n",
            "Epoch 59/199\n",
            "----------\n",
            "train Loss: 0.1072 Acc: 0.9688\n",
            "val Loss: 1.9512 Acc: 0.6111\n",
            "Epoch 60/199\n",
            "----------\n",
            "train Loss: 0.0789 Acc: 0.9782\n",
            "val Loss: 1.8914 Acc: 0.6199\n",
            "Epoch 61/199\n",
            "----------\n",
            "train Loss: 0.0633 Acc: 0.9831\n",
            "val Loss: 1.8614 Acc: 0.6241\n",
            "Epoch 62/199\n",
            "----------\n",
            "train Loss: 0.0597 Acc: 0.9843\n",
            "val Loss: 1.8606 Acc: 0.6257\n",
            "Epoch 63/199\n",
            "----------\n",
            "train Loss: 0.0533 Acc: 0.9862\n",
            "val Loss: 1.8523 Acc: 0.6264\n",
            "Epoch 64/199\n",
            "----------\n",
            "train Loss: 0.0516 Acc: 0.9869\n",
            "val Loss: 1.8621 Acc: 0.6281\n",
            "Epoch 65/199\n",
            "----------\n",
            "train Loss: 0.0478 Acc: 0.9880\n",
            "val Loss: 1.8516 Acc: 0.6281\n",
            "Epoch 66/199\n",
            "----------\n",
            "train Loss: 0.0466 Acc: 0.9887\n",
            "val Loss: 1.8572 Acc: 0.6297\n",
            "Epoch 67/199\n",
            "----------\n",
            "train Loss: 0.0422 Acc: 0.9904\n",
            "val Loss: 1.8491 Acc: 0.6323\n",
            "Epoch 68/199\n",
            "----------\n",
            "train Loss: 0.0424 Acc: 0.9898\n",
            "val Loss: 1.8509 Acc: 0.6275\n",
            "Epoch 69/199\n",
            "----------\n",
            "train Loss: 0.0415 Acc: 0.9903\n",
            "val Loss: 1.8576 Acc: 0.6281\n",
            "Epoch 70/199\n",
            "----------\n",
            "train Loss: 0.0387 Acc: 0.9904\n",
            "val Loss: 1.8517 Acc: 0.6305\n",
            "Epoch 71/199\n",
            "----------\n",
            "train Loss: 0.0405 Acc: 0.9902\n",
            "val Loss: 1.8559 Acc: 0.6283\n",
            "Epoch 72/199\n",
            "----------\n",
            "train Loss: 0.0363 Acc: 0.9911\n",
            "val Loss: 1.8603 Acc: 0.6262\n",
            "Epoch 73/199\n",
            "----------\n",
            "train Loss: 0.0347 Acc: 0.9920\n",
            "val Loss: 1.8572 Acc: 0.6294\n",
            "Epoch 74/199\n",
            "----------\n",
            "train Loss: 0.0351 Acc: 0.9914\n",
            "val Loss: 1.8549 Acc: 0.6285\n",
            "Epoch 75/199\n",
            "----------\n",
            "train Loss: 0.0347 Acc: 0.9918\n",
            "val Loss: 1.8601 Acc: 0.6287\n",
            "Epoch 76/199\n",
            "----------\n",
            "train Loss: 0.0344 Acc: 0.9917\n",
            "val Loss: 1.8654 Acc: 0.6284\n",
            "Epoch 77/199\n",
            "----------\n",
            "train Loss: 0.0352 Acc: 0.9914\n",
            "val Loss: 1.8588 Acc: 0.6332\n",
            "Epoch 78/199\n",
            "----------\n",
            "train Loss: 0.0316 Acc: 0.9926\n",
            "val Loss: 1.8597 Acc: 0.6295\n",
            "Epoch 79/199\n",
            "----------\n",
            "train Loss: 0.0321 Acc: 0.9929\n",
            "val Loss: 1.8584 Acc: 0.6308\n",
            "Epoch 80/199\n",
            "----------\n",
            "train Loss: 0.0310 Acc: 0.9935\n",
            "val Loss: 1.8570 Acc: 0.6320\n",
            "Epoch 81/199\n",
            "----------\n",
            "train Loss: 0.0309 Acc: 0.9929\n",
            "val Loss: 1.8684 Acc: 0.6320\n",
            "Epoch 82/199\n",
            "----------\n",
            "train Loss: 0.0298 Acc: 0.9929\n",
            "val Loss: 1.8646 Acc: 0.6307\n",
            "Epoch 83/199\n",
            "----------\n",
            "train Loss: 0.0293 Acc: 0.9932\n",
            "val Loss: 1.8572 Acc: 0.6347\n",
            "Epoch 84/199\n",
            "----------\n",
            "train Loss: 0.0293 Acc: 0.9935\n",
            "val Loss: 1.8514 Acc: 0.6322\n",
            "Epoch 85/199\n",
            "----------\n",
            "train Loss: 0.0307 Acc: 0.9926\n",
            "val Loss: 1.8745 Acc: 0.6297\n",
            "Epoch 86/199\n",
            "----------\n",
            "train Loss: 0.0278 Acc: 0.9937\n",
            "val Loss: 1.8635 Acc: 0.6311\n",
            "Epoch 87/199\n",
            "----------\n",
            "train Loss: 0.0276 Acc: 0.9935\n",
            "val Loss: 1.8770 Acc: 0.6304\n",
            "Epoch 88/199\n",
            "----------\n",
            "train Loss: 0.0283 Acc: 0.9935\n",
            "val Loss: 1.8720 Acc: 0.6310\n",
            "Epoch 89/199\n",
            "----------\n",
            "train Loss: 0.0286 Acc: 0.9935\n",
            "val Loss: 1.8733 Acc: 0.6279\n",
            "Epoch 90/199\n",
            "----------\n",
            "train Loss: 0.0265 Acc: 0.9942\n",
            "val Loss: 1.8722 Acc: 0.6308\n",
            "Epoch 91/199\n",
            "----------\n",
            "train Loss: 0.0267 Acc: 0.9939\n",
            "val Loss: 1.8691 Acc: 0.6293\n",
            "Epoch 92/199\n",
            "----------\n",
            "train Loss: 0.0254 Acc: 0.9946\n",
            "val Loss: 1.8778 Acc: 0.6286\n",
            "Epoch 93/199\n",
            "----------\n",
            "train Loss: 0.0260 Acc: 0.9943\n",
            "val Loss: 1.8725 Acc: 0.6302\n",
            "Epoch 94/199\n",
            "----------\n",
            "train Loss: 0.0255 Acc: 0.9945\n",
            "val Loss: 1.8782 Acc: 0.6294\n",
            "Epoch 95/199\n",
            "----------\n",
            "train Loss: 0.0253 Acc: 0.9942\n",
            "val Loss: 1.8975 Acc: 0.6299\n",
            "Epoch 96/199\n",
            "----------\n",
            "train Loss: 0.0239 Acc: 0.9950\n",
            "val Loss: 1.8741 Acc: 0.6306\n",
            "Epoch 97/199\n",
            "----------\n",
            "train Loss: 0.0248 Acc: 0.9943\n",
            "val Loss: 1.8799 Acc: 0.6279\n",
            "Epoch 98/199\n",
            "----------\n",
            "train Loss: 0.0249 Acc: 0.9943\n",
            "val Loss: 1.8747 Acc: 0.6327\n",
            "Epoch 99/199\n",
            "----------\n",
            "train Loss: 0.0241 Acc: 0.9948\n",
            "val Loss: 1.8751 Acc: 0.6313\n",
            "Epoch 100/199\n",
            "----------\n",
            "train Loss: 0.0232 Acc: 0.9951\n",
            "val Loss: 1.8813 Acc: 0.6300\n",
            "Epoch 101/199\n",
            "----------\n",
            "train Loss: 0.0243 Acc: 0.9944\n",
            "val Loss: 1.8721 Acc: 0.6345\n",
            "Epoch 102/199\n",
            "----------\n",
            "train Loss: 0.0240 Acc: 0.9941\n",
            "val Loss: 1.8939 Acc: 0.6301\n",
            "Epoch 103/199\n",
            "----------\n",
            "train Loss: 0.0249 Acc: 0.9944\n",
            "val Loss: 1.8891 Acc: 0.6322\n",
            "Epoch 104/199\n",
            "----------\n",
            "train Loss: 0.0222 Acc: 0.9954\n",
            "val Loss: 1.8812 Acc: 0.6285\n",
            "Epoch 105/199\n",
            "----------\n",
            "train Loss: 0.0228 Acc: 0.9949\n",
            "val Loss: 1.8814 Acc: 0.6317\n",
            "Epoch 106/199\n",
            "----------\n",
            "train Loss: 0.0209 Acc: 0.9961\n",
            "val Loss: 1.8896 Acc: 0.6283\n",
            "Epoch 107/199\n",
            "----------\n",
            "train Loss: 0.0230 Acc: 0.9949\n",
            "val Loss: 1.8873 Acc: 0.6304\n",
            "Epoch 108/199\n",
            "----------\n",
            "train Loss: 0.0235 Acc: 0.9950\n",
            "val Loss: 1.8852 Acc: 0.6316\n",
            "Epoch 109/199\n",
            "----------\n",
            "train Loss: 0.0224 Acc: 0.9953\n",
            "val Loss: 1.8839 Acc: 0.6300\n",
            "Epoch 110/199\n",
            "----------\n",
            "train Loss: 0.0223 Acc: 0.9953\n",
            "val Loss: 1.8999 Acc: 0.6312\n",
            "Epoch 111/199\n",
            "----------\n",
            "train Loss: 0.0214 Acc: 0.9958\n",
            "val Loss: 1.8838 Acc: 0.6346\n",
            "Epoch 112/199\n",
            "----------\n",
            "train Loss: 0.0198 Acc: 0.9959\n",
            "val Loss: 1.8908 Acc: 0.6306\n",
            "Epoch 113/199\n",
            "----------\n",
            "train Loss: 0.0210 Acc: 0.9955\n",
            "val Loss: 1.8984 Acc: 0.6320\n",
            "Epoch 114/199\n",
            "----------\n",
            "train Loss: 0.0203 Acc: 0.9962\n",
            "val Loss: 1.8879 Acc: 0.6337\n",
            "Epoch 115/199\n",
            "----------\n",
            "train Loss: 0.0208 Acc: 0.9956\n",
            "val Loss: 1.9009 Acc: 0.6315\n",
            "Epoch 116/199\n",
            "----------\n",
            "train Loss: 0.0208 Acc: 0.9958\n",
            "val Loss: 1.8995 Acc: 0.6284\n",
            "Epoch 117/199\n",
            "----------\n",
            "train Loss: 0.0190 Acc: 0.9961\n",
            "val Loss: 1.9009 Acc: 0.6277\n",
            "Epoch 118/199\n",
            "----------\n",
            "train Loss: 0.0199 Acc: 0.9961\n",
            "val Loss: 1.8964 Acc: 0.6304\n",
            "Epoch 119/199\n",
            "----------\n",
            "train Loss: 0.0198 Acc: 0.9959\n",
            "val Loss: 1.8966 Acc: 0.6305\n",
            "Epoch 120/199\n",
            "----------\n",
            "train Loss: 0.0193 Acc: 0.9959\n",
            "val Loss: 1.8991 Acc: 0.6307\n",
            "Epoch 121/199\n",
            "----------\n",
            "train Loss: 0.0191 Acc: 0.9962\n",
            "val Loss: 1.8887 Acc: 0.6306\n",
            "Epoch 122/199\n",
            "----------\n",
            "train Loss: 0.0195 Acc: 0.9958\n",
            "val Loss: 1.8914 Acc: 0.6295\n",
            "Epoch 123/199\n",
            "----------\n",
            "train Loss: 0.0193 Acc: 0.9958\n",
            "val Loss: 1.8988 Acc: 0.6325\n",
            "Epoch 124/199\n",
            "----------\n",
            "train Loss: 0.0189 Acc: 0.9963\n",
            "val Loss: 1.8899 Acc: 0.6285\n",
            "Epoch 125/199\n",
            "----------\n",
            "train Loss: 0.0204 Acc: 0.9955\n",
            "val Loss: 1.8901 Acc: 0.6309\n",
            "Epoch 126/199\n",
            "----------\n",
            "train Loss: 0.0188 Acc: 0.9963\n",
            "val Loss: 1.8940 Acc: 0.6328\n",
            "Epoch 127/199\n",
            "----------\n",
            "train Loss: 0.0191 Acc: 0.9959\n",
            "val Loss: 1.8978 Acc: 0.6311\n",
            "Epoch 128/199\n",
            "----------\n",
            "train Loss: 0.0195 Acc: 0.9959\n",
            "val Loss: 1.8900 Acc: 0.6323\n",
            "Epoch 129/199\n",
            "----------\n",
            "train Loss: 0.0185 Acc: 0.9964\n",
            "val Loss: 1.8976 Acc: 0.6304\n",
            "Epoch 130/199\n",
            "----------\n",
            "train Loss: 0.0187 Acc: 0.9960\n",
            "val Loss: 1.8939 Acc: 0.6325\n",
            "Epoch 131/199\n",
            "----------\n",
            "train Loss: 0.0192 Acc: 0.9957\n",
            "val Loss: 1.8931 Acc: 0.6300\n",
            "Epoch 132/199\n",
            "----------\n",
            "train Loss: 0.0196 Acc: 0.9958\n",
            "val Loss: 1.8922 Acc: 0.6291\n",
            "Epoch 133/199\n",
            "----------\n",
            "train Loss: 0.0207 Acc: 0.9951\n",
            "val Loss: 1.8994 Acc: 0.6320\n",
            "Epoch 134/199\n",
            "----------\n",
            "train Loss: 0.0191 Acc: 0.9958\n",
            "val Loss: 1.8998 Acc: 0.6316\n",
            "Epoch 135/199\n",
            "----------\n",
            "train Loss: 0.0188 Acc: 0.9963\n",
            "val Loss: 1.8879 Acc: 0.6300\n",
            "Epoch 136/199\n",
            "----------\n",
            "train Loss: 0.0188 Acc: 0.9962\n",
            "val Loss: 1.8941 Acc: 0.6307\n",
            "Epoch 137/199\n",
            "----------\n",
            "train Loss: 0.0183 Acc: 0.9963\n",
            "val Loss: 1.8985 Acc: 0.6311\n",
            "Epoch 138/199\n",
            "----------\n",
            "train Loss: 0.0188 Acc: 0.9959\n",
            "val Loss: 1.8978 Acc: 0.6293\n",
            "Epoch 139/199\n",
            "----------\n",
            "train Loss: 0.0190 Acc: 0.9959\n",
            "val Loss: 1.8853 Acc: 0.6323\n",
            "Epoch 140/199\n",
            "----------\n",
            "train Loss: 0.0190 Acc: 0.9961\n",
            "val Loss: 1.8881 Acc: 0.6323\n",
            "Epoch 141/199\n",
            "----------\n",
            "train Loss: 0.0193 Acc: 0.9961\n",
            "val Loss: 1.9015 Acc: 0.6304\n",
            "Epoch 142/199\n",
            "----------\n",
            "train Loss: 0.0188 Acc: 0.9963\n",
            "val Loss: 1.8949 Acc: 0.6322\n",
            "Epoch 143/199\n",
            "----------\n",
            "train Loss: 0.0194 Acc: 0.9957\n",
            "val Loss: 1.8952 Acc: 0.6320\n",
            "Epoch 144/199\n",
            "----------\n",
            "train Loss: 0.0187 Acc: 0.9961\n",
            "val Loss: 1.8960 Acc: 0.6313\n",
            "Epoch 145/199\n",
            "----------\n",
            "train Loss: 0.0186 Acc: 0.9962\n",
            "val Loss: 1.8999 Acc: 0.6290\n",
            "Epoch 146/199\n",
            "----------\n",
            "train Loss: 0.0195 Acc: 0.9957\n",
            "val Loss: 1.8956 Acc: 0.6295\n",
            "Epoch 147/199\n",
            "----------\n",
            "train Loss: 0.0192 Acc: 0.9960\n",
            "val Loss: 1.8997 Acc: 0.6296\n",
            "Epoch 148/199\n",
            "----------\n",
            "train Loss: 0.0203 Acc: 0.9958\n",
            "val Loss: 1.8905 Acc: 0.6291\n",
            "Epoch 149/199\n",
            "----------\n",
            "train Loss: 0.0191 Acc: 0.9958\n",
            "val Loss: 1.9038 Acc: 0.6278\n",
            "Epoch 150/199\n",
            "----------\n",
            "train Loss: 0.0183 Acc: 0.9962\n",
            "val Loss: 1.9095 Acc: 0.6307\n",
            "Epoch 151/199\n",
            "----------\n",
            "train Loss: 0.0187 Acc: 0.9960\n",
            "val Loss: 1.8998 Acc: 0.6318\n",
            "Epoch 152/199\n",
            "----------\n",
            "train Loss: 0.0184 Acc: 0.9961\n",
            "val Loss: 1.8915 Acc: 0.6297\n",
            "Epoch 153/199\n",
            "----------\n",
            "train Loss: 0.0185 Acc: 0.9961\n",
            "val Loss: 1.8885 Acc: 0.6309\n",
            "Epoch 154/199\n",
            "----------\n",
            "train Loss: 0.0177 Acc: 0.9966\n",
            "val Loss: 1.8931 Acc: 0.6308\n",
            "Epoch 155/199\n",
            "----------\n",
            "train Loss: 0.0186 Acc: 0.9960\n",
            "val Loss: 1.8887 Acc: 0.6309\n",
            "Epoch 156/199\n",
            "----------\n",
            "train Loss: 0.0189 Acc: 0.9957\n",
            "val Loss: 1.9072 Acc: 0.6305\n",
            "Epoch 157/199\n",
            "----------\n",
            "train Loss: 0.0192 Acc: 0.9959\n",
            "val Loss: 1.8862 Acc: 0.6283\n",
            "Epoch 158/199\n",
            "----------\n",
            "train Loss: 0.0199 Acc: 0.9956\n",
            "val Loss: 1.9034 Acc: 0.6332\n",
            "Epoch 159/199\n",
            "----------\n",
            "train Loss: 0.0182 Acc: 0.9964\n",
            "val Loss: 1.8996 Acc: 0.6308\n",
            "Epoch 160/199\n",
            "----------\n",
            "train Loss: 0.0188 Acc: 0.9959\n",
            "val Loss: 1.9077 Acc: 0.6287\n",
            "Epoch 161/199\n",
            "----------\n",
            "train Loss: 0.0181 Acc: 0.9965\n",
            "val Loss: 1.9002 Acc: 0.6311\n",
            "Epoch 162/199\n",
            "----------\n",
            "train Loss: 0.0186 Acc: 0.9960\n",
            "val Loss: 1.8879 Acc: 0.6340\n",
            "Epoch 163/199\n",
            "----------\n",
            "train Loss: 0.0178 Acc: 0.9964\n",
            "val Loss: 1.8927 Acc: 0.6330\n",
            "Epoch 164/199\n",
            "----------\n",
            "train Loss: 0.0183 Acc: 0.9962\n",
            "val Loss: 1.8847 Acc: 0.6291\n",
            "Epoch 165/199\n",
            "----------\n",
            "train Loss: 0.0184 Acc: 0.9962\n",
            "val Loss: 1.8936 Acc: 0.6319\n",
            "Epoch 166/199\n",
            "----------\n",
            "train Loss: 0.0189 Acc: 0.9959\n",
            "val Loss: 1.8997 Acc: 0.6307\n",
            "Epoch 167/199\n",
            "----------\n",
            "train Loss: 0.0173 Acc: 0.9966\n",
            "val Loss: 1.8845 Acc: 0.6328\n",
            "Epoch 168/199\n",
            "----------\n",
            "train Loss: 0.0184 Acc: 0.9964\n",
            "val Loss: 1.8963 Acc: 0.6306\n",
            "Epoch 169/199\n",
            "----------\n",
            "train Loss: 0.0184 Acc: 0.9961\n",
            "val Loss: 1.8896 Acc: 0.6291\n",
            "Epoch 170/199\n",
            "----------\n",
            "train Loss: 0.0182 Acc: 0.9962\n",
            "val Loss: 1.9018 Acc: 0.6297\n",
            "Epoch 171/199\n",
            "----------\n",
            "train Loss: 0.0180 Acc: 0.9962\n",
            "val Loss: 1.8973 Acc: 0.6302\n",
            "Epoch 172/199\n",
            "----------\n",
            "train Loss: 0.0179 Acc: 0.9966\n",
            "val Loss: 1.8987 Acc: 0.6283\n",
            "Epoch 173/199\n",
            "----------\n",
            "train Loss: 0.0180 Acc: 0.9964\n",
            "val Loss: 1.8897 Acc: 0.6334\n",
            "Epoch 174/199\n",
            "----------\n",
            "train Loss: 0.0180 Acc: 0.9963\n",
            "val Loss: 1.9040 Acc: 0.6300\n",
            "Epoch 175/199\n",
            "----------\n",
            "train Loss: 0.0179 Acc: 0.9963\n",
            "val Loss: 1.9029 Acc: 0.6267\n",
            "Epoch 176/199\n",
            "----------\n",
            "train Loss: 0.0190 Acc: 0.9958\n",
            "val Loss: 1.8903 Acc: 0.6311\n",
            "Epoch 177/199\n",
            "----------\n",
            "train Loss: 0.0180 Acc: 0.9963\n",
            "val Loss: 1.9081 Acc: 0.6284\n",
            "Epoch 178/199\n",
            "----------\n",
            "train Loss: 0.0189 Acc: 0.9961\n",
            "val Loss: 1.8881 Acc: 0.6313\n",
            "Epoch 179/199\n",
            "----------\n",
            "train Loss: 0.0176 Acc: 0.9962\n",
            "val Loss: 1.8945 Acc: 0.6297\n",
            "Epoch 180/199\n",
            "----------\n",
            "train Loss: 0.0180 Acc: 0.9966\n",
            "val Loss: 1.8909 Acc: 0.6305\n",
            "Epoch 181/199\n",
            "----------\n",
            "train Loss: 0.0180 Acc: 0.9963\n",
            "val Loss: 1.8974 Acc: 0.6319\n",
            "Epoch 182/199\n",
            "----------\n",
            "train Loss: 0.0176 Acc: 0.9962\n",
            "val Loss: 1.8942 Acc: 0.6291\n",
            "Epoch 183/199\n",
            "----------\n",
            "train Loss: 0.0182 Acc: 0.9962\n",
            "val Loss: 1.8968 Acc: 0.6312\n",
            "Epoch 184/199\n",
            "----------\n",
            "train Loss: 0.0183 Acc: 0.9961\n",
            "val Loss: 1.8980 Acc: 0.6300\n",
            "Epoch 185/199\n",
            "----------\n",
            "train Loss: 0.0181 Acc: 0.9963\n",
            "val Loss: 1.8896 Acc: 0.6310\n",
            "Epoch 186/199\n",
            "----------\n",
            "train Loss: 0.0176 Acc: 0.9963\n",
            "val Loss: 1.8959 Acc: 0.6327\n",
            "Epoch 187/199\n",
            "----------\n",
            "train Loss: 0.0190 Acc: 0.9959\n",
            "val Loss: 1.8989 Acc: 0.6311\n",
            "Epoch 188/199\n",
            "----------\n",
            "train Loss: 0.0179 Acc: 0.9962\n",
            "val Loss: 1.9068 Acc: 0.6299\n",
            "Epoch 189/199\n",
            "----------\n",
            "train Loss: 0.0180 Acc: 0.9961\n",
            "val Loss: 1.8997 Acc: 0.6324\n",
            "Epoch 190/199\n",
            "----------\n",
            "train Loss: 0.0175 Acc: 0.9964\n",
            "val Loss: 1.8978 Acc: 0.6312\n",
            "Epoch 191/199\n",
            "----------\n",
            "train Loss: 0.0180 Acc: 0.9964\n",
            "val Loss: 1.9007 Acc: 0.6325\n",
            "Epoch 192/199\n",
            "----------\n",
            "train Loss: 0.0173 Acc: 0.9964\n",
            "val Loss: 1.9023 Acc: 0.6289\n",
            "Epoch 193/199\n",
            "----------\n",
            "train Loss: 0.0178 Acc: 0.9960\n",
            "val Loss: 1.8902 Acc: 0.6299\n",
            "Epoch 194/199\n",
            "----------\n",
            "train Loss: 0.0179 Acc: 0.9964\n",
            "val Loss: 1.8922 Acc: 0.6323\n",
            "Epoch 195/199\n",
            "----------\n",
            "train Loss: 0.0182 Acc: 0.9965\n",
            "val Loss: 1.9012 Acc: 0.6303\n",
            "Epoch 196/199\n",
            "----------\n",
            "train Loss: 0.0179 Acc: 0.9962\n",
            "val Loss: 1.8996 Acc: 0.6288\n",
            "Epoch 197/199\n",
            "----------\n",
            "train Loss: 0.0192 Acc: 0.9959\n",
            "val Loss: 1.8994 Acc: 0.6312\n",
            "Epoch 198/199\n",
            "----------\n",
            "train Loss: 0.0176 Acc: 0.9967\n",
            "val Loss: 1.8893 Acc: 0.6317\n",
            "Epoch 199/199\n",
            "----------\n",
            "train Loss: 0.0179 Acc: 0.9963\n",
            "val Loss: 1.8813 Acc: 0.6323\n",
            "Best val Acc: 0.634700\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
